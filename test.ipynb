{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaf9bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from utils import * \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a023c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths\n",
    "ROOT = Path(\"Amazon_products\") # Root Amazon_products directory\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "TRAIN_CORPUS_PATH = os.path.join(TRAIN_DIR, \"train_corpus.txt\")\n",
    "\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\" \n",
    "CLASS_RELATED_PATH = ROOT / \"class_related_keywords.txt\" \n",
    "CLASS_PATH = ROOT / \"classes.txt\" \n",
    "\n",
    "SUBMISSION_PATH = \"Submission/submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0–530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cf9bdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 29487 samples\n",
      "Test: 19658 samples\n",
      "Classes: 531\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# Default paths\n",
    "ROOT = Path(\"Amazon_products\") # Root Amazon_products directory\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "TRAIN_CORPUS_PATH = os.path.join(TRAIN_DIR, \"train_corpus.txt\")\n",
    "\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\" \n",
    "CLASS_RELATED_PATH = ROOT / \"class_related_keywords.txt\" \n",
    "CLASS_PATH = ROOT / \"classes.txt\" \n",
    "\n",
    "SUBMISSION_PATH = \"Submission/submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0–530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# Load corpus\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {id: text} dictionary.\"\"\"\n",
    "    id2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                id, text = parts\n",
    "                id2text[id] = text\n",
    "    return id2text\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"Load multi-label data into {id: [labels]} dictionary.\"\"\"\n",
    "    id2labels = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, label = parts\n",
    "                pid = int(pid)\n",
    "                label = int(label)\n",
    "\n",
    "                if pid not in id2labels:\n",
    "                    id2labels[pid] = []\n",
    "\n",
    "                id2labels[pid].append(label)\n",
    "    return id2labels\n",
    "\n",
    "def load_class_keywords(path):\n",
    "    \"\"\"Load class keywords into {class_name: [keywords]} dictionary.\"\"\"\n",
    "    class2keywords = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            classname, keywords = line.strip().split(\":\", 1)\n",
    "            keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
    "            class2keywords[classname] = keyword_list\n",
    "    return class2keywords\n",
    "\n",
    "id2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "id2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "\n",
    "id2class = load_corpus(CLASS_PATH)\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "class2related = load_class_keywords(CLASS_RELATED_PATH)\n",
    "\n",
    "# Load silver labels\n",
    "with open(\"Silver/silver_train_roberta.json\", \"r\") as f:\n",
    "    pid2labelids_silver = json.load(f)\n",
    "\n",
    "with open(\"Silver/silver_test_roberta.json\", \"r\") as f:\n",
    "    pid2labelids_test = json.load(f)\n",
    "\n",
    "print(f\"Train: {len(id2text_train)} samples\")\n",
    "print(f\"Test: {len(id2text_test)} samples\")\n",
    "print(f\"Classes: {len(id2class)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83c05194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained embeddings...\n",
      "Train embeddings: torch.Size([29487, 768])\n",
      "Test embeddings:  torch.Size([19658, 768])\n",
      "Train labels:     torch.Size([29487, 531])\n",
      "Label Emb:     torch.Size([531, 768])\n",
      "Input dimension: 768\n",
      "Num classes: 531\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Paths ===\n",
    "X_TRAIN_PATH = \"Embeddings/X_train.pt\"\n",
    "Y_TRAIN_PATH = \"Embeddings/y_train.pt\"\n",
    "X_TEST_PATH  = \"Embeddings/X_test.pt\"\n",
    "TRAIN_IDS_PATH = \"Embeddings/train_ids.pt\"\n",
    "TEST_IDS_PATH  = \"Embeddings/test_ids.pt\"\n",
    "LABEL_EMB_PATH = \"Embeddings/label_emb.pt\"\n",
    "\n",
    "# === Load pre-computed embeddings ===\n",
    "print(\"Loading pre-trained embeddings...\")\n",
    "X_train = torch.load(X_TRAIN_PATH).to(device)\n",
    "y_train = torch.load(Y_TRAIN_PATH).to(device)\n",
    "X_test  = torch.load(X_TEST_PATH).to(device)\n",
    "label_emb  = torch.load(LABEL_EMB_PATH).to(device)\n",
    "\n",
    "test_ids = list(id2text_test.keys())\n",
    "train_ids = list(id2text_train.keys())\n",
    "\n",
    "print(f\"Train embeddings: {X_train.shape}\")\n",
    "print(f\"Test embeddings:  {X_test.shape}\")\n",
    "print(f\"Train labels:     {y_train.shape}\")\n",
    "print(f\"Label Emb:     {label_emb.shape}\")\n",
    "\n",
    "# === Build index for convenience ===\n",
    "pid2idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "# === Some useful info ===\n",
    "input_dim = X_train.size(1)\n",
    "num_classes = y_train.size(1)\n",
    "\n",
    "print(\"Input dimension:\", input_dim)\n",
    "print(\"Num classes:\", num_classes)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bb01ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier that uses label embeddings to make predictions\n",
    "class InnerProductClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, label_embeddings, dropout=0.2, trainable_label_emb=False):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) # => for consistency\n",
    "        # Project input features into the same dimension as label embeddings\n",
    "        self.proj = nn.Linear(input_dim, label_embeddings.size(1))\n",
    "\n",
    "        if trainable_label_emb:\n",
    "            # Label embeddings are trainable parameters\n",
    "            self.label_emb = nn.Parameter(label_embeddings.clone())\n",
    "        else:\n",
    "            # Label embeddings are fixed (not updated during training)\n",
    "            self.register_buffer(\"label_emb\", label_embeddings.clone())\n",
    "\n",
    "    def forward(self, x, use_dropout=True):\n",
    "        if use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        # Project input feature vectors\n",
    "        x_proj = self.proj(x)\n",
    "        # Compute logits as similarity with each label embedding\n",
    "        logits = torch.matmul(x_proj, self.label_emb.T)\n",
    "        return logits\n",
    "    \n",
    "class ProductCategoryEmbeddingDataset(Dataset):\n",
    "    def __init__(self, pid2label, pid2idx, embeddings, num_classes=531):\n",
    "        self.pids = list(pid2label.keys())\n",
    "        self.labels = [pid2label[pid] for pid in self.pids]\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]\n",
    "        self.embeddings = embeddings\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[self.indices[idx]]\n",
    "        \n",
    "        y = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for label in self.labels[idx]:\n",
    "            y[label] = 1.0\n",
    "\n",
    "        return {\"X\": emb, \"y\": y}\n",
    "    \n",
    "class TensorDatasetFromVectors(Dataset):\n",
    "    def __init__(self, X_list, y_list):\n",
    "        self.X = torch.stack(X_list)                     # list of embeddings -> tensor\n",
    "        self.y = torch.stack(y_list).float()             # list of binary vectors -> tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"X\": self.X[idx], \"y\": self.y[idx]}      # embedding + multi-label\n",
    "\n",
    "class UnlabeledEmbeddingDataset(Dataset):\n",
    "    def __init__(self, pids, pid2idx, embeddings):\n",
    "        self.pids = pids\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[self.indices[idx]]\n",
    "        pid = self.pids[idx]\n",
    "        return {\"X\": emb, \"pid\": pid}\n",
    "    \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\", threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].cpu().numpy()\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs > threshold).astype(int)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y)\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    return {\"f1_macro\": f1_macro, \"f1_micro\": f1_micro}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "824ae0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 25063 | Val: 4424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Split des silver labels en train/val (85/15) ---\n",
    "silver_pids = list(pid2labelids_silver.keys())  # tu as déjà chargé pid2labelids_silver\n",
    "train_pids, val_pids = train_test_split(\n",
    "    silver_pids, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "train_labels = {pid: pid2labelids_silver[pid] for pid in train_pids}\n",
    "val_labels   = {pid: pid2labelids_silver[pid] for pid in val_pids}\n",
    "\n",
    "print(f\"Train: {len(train_labels)} | Val: {len(val_labels)}\")\n",
    "\n",
    "# --- Datasets (basés sur tes embeddings Roberta déjà chargés) ---\n",
    "train_dataset = ProductCategoryEmbeddingDataset(train_labels, pid2idx, X_train, num_classes=NUM_CLASSES)\n",
    "val_dataset   = ProductCategoryEmbeddingDataset(val_labels, pid2idx, X_train, num_classes=NUM_CLASSES)\n",
    "\n",
    "# --- DataLoaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# --- Test set ---\n",
    "test_dataset = ProductCategoryEmbeddingDataset(pid2labelids_test, pid2idx, X_test, num_classes=NUM_CLASSES)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "804d8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Training (InnerProductClassifier + BCE) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/392 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=0.0385 | val_f1_macro=0.0016\n",
      "Best model updated (val_f1=0.0016)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.0183 | val_f1_macro=0.0050\n",
      "Best model updated (val_f1=0.0050)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] train_loss=0.0157 | val_f1_macro=0.0068\n",
      "Best model updated (val_f1=0.0068)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] train_loss=0.0145 | val_f1_macro=0.0083\n",
      "Best model updated (val_f1=0.0083)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] train_loss=0.0137 | val_f1_macro=0.0089\n",
      "Best model updated (val_f1=0.0089)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] train_loss=0.0132 | val_f1_macro=0.0106\n",
      "Best model updated (val_f1=0.0106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] train_loss=0.0128 | val_f1_macro=0.0109\n",
      "Best model updated (val_f1=0.0109)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] train_loss=0.0125 | val_f1_macro=0.0118\n",
      "Best model updated (val_f1=0.0118)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] train_loss=0.0122 | val_f1_macro=0.0123\n",
      "Best model updated (val_f1=0.0123)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss=0.0120 | val_f1_macro=0.0140\n",
      "Best model updated (val_f1=0.0140)\n",
      "\n",
      "=== Final Test (Best Model) ===\n",
      "{'f1_macro': 0.014158797269227327, 'f1_micro': 0.5700567714159087}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Initialisation du modèle ===\n",
    "student = InnerProductClassifier(input_dim, label_emb).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=60)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EPOCHS_BASE = 10\n",
    "patience = 5\n",
    "wait = 0\n",
    "best_f1 = 0.0\n",
    "best_model = copy.deepcopy(student.state_dict())\n",
    "\n",
    "# === Training loop ===\n",
    "def train_epoch(loader):\n",
    "    student.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        logits = student(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def val_f1():\n",
    "    student.eval()\n",
    "    return evaluate(student, val_loader, device)['f1_macro']\n",
    "\n",
    "print(\"\\n=== Baseline Training (InnerProductClassifier + BCE) ===\")\n",
    "for epoch in range(1, EPOCHS_BASE + 1):\n",
    "    train_loss = train_epoch(train_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    f1_val = val_f1()\n",
    "    print(f\"[Epoch {epoch}] train_loss={train_loss:.4f} | val_f1_macro={f1_val:.4f}\")\n",
    "\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        wait = 0\n",
    "        best_model = copy.deepcopy(student.state_dict())\n",
    "        print(f\"Best model updated (val_f1={best_f1:.4f})\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        print(f\"No improvement: {wait}/{patience}\")\n",
    "\n",
    "    if wait >= patience:\n",
    "        print(\"\\nEarly stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# === Test final ===\n",
    "student.load_state_dict(best_model)\n",
    "print(\"\\n=== Final Test (Best Model) ===\")\n",
    "test_result = evaluate(student, test_loader, device)\n",
    "print(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32b11a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 392/392 [00:01<00:00, 248.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.0736\n",
      "F1-macro=0.0011 | F1-micro=0.1561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 392/392 [00:01<00:00, 336.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=0.0165\n",
      "F1-macro=0.0030 | F1-micro=0.3271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 392/392 [00:01<00:00, 321.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.0152\n",
      "F1-macro=0.0052 | F1-micro=0.4130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 392/392 [00:01<00:00, 301.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss=0.0142\n",
      "F1-macro=0.0061 | F1-micro=0.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 392/392 [00:01<00:00, 302.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss=0.0136\n",
      "F1-macro=0.0069 | F1-micro=0.5007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 392/392 [00:01<00:00, 302.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss=0.0132\n",
      "F1-macro=0.0073 | F1-micro=0.5092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 392/392 [00:01<00:00, 293.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss=0.0129\n",
      "F1-macro=0.0078 | F1-micro=0.5158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 392/392 [00:01<00:00, 311.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss=0.0126\n",
      "F1-macro=0.0082 | F1-micro=0.5242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 392/392 [00:01<00:00, 298.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss=0.0123\n",
      "F1-macro=0.0089 | F1-micro=0.5334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 392/392 [00:01<00:00, 299.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss=0.0121\n",
      "F1-macro=0.0096 | F1-micro=0.5440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 392/392 [00:01<00:00, 301.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss=0.0119\n",
      "F1-macro=0.0101 | F1-micro=0.5512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 392/392 [00:01<00:00, 298.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss=0.0117\n",
      "F1-macro=0.0106 | F1-micro=0.5531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 392/392 [00:01<00:00, 296.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss=0.0115\n",
      "F1-macro=0.0113 | F1-micro=0.5586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 392/392 [00:01<00:00, 299.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss=0.0113\n",
      "F1-macro=0.0117 | F1-micro=0.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 392/392 [00:01<00:00, 296.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss=0.0112\n",
      "F1-macro=0.0123 | F1-micro=0.5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 392/392 [00:01<00:00, 292.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss=0.0111\n",
      "F1-macro=0.0125 | F1-micro=0.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 392/392 [00:01<00:00, 296.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss=0.0110\n",
      "F1-macro=0.0130 | F1-micro=0.5726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 392/392 [00:01<00:00, 261.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss=0.0108\n",
      "F1-macro=0.0140 | F1-micro=0.5782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 392/392 [00:01<00:00, 277.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss=0.0107\n",
      "F1-macro=0.0143 | F1-micro=0.5822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 392/392 [00:01<00:00, 292.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss=0.0106\n",
      "F1-macro=0.0149 | F1-micro=0.5834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 392/392 [00:01<00:00, 297.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: loss=0.0105\n",
      "F1-macro=0.0156 | F1-micro=0.5883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 392/392 [00:01<00:00, 280.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: loss=0.0104\n",
      "F1-macro=0.0155 | F1-micro=0.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 392/392 [00:01<00:00, 301.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: loss=0.0103\n",
      "F1-macro=0.0165 | F1-micro=0.5908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 392/392 [00:01<00:00, 305.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: loss=0.0102\n",
      "F1-macro=0.0174 | F1-micro=0.5920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 392/392 [00:01<00:00, 298.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: loss=0.0102\n",
      "F1-macro=0.0178 | F1-micro=0.5967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 392/392 [00:01<00:00, 297.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: loss=0.0101\n",
      "F1-macro=0.0178 | F1-micro=0.5974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 392/392 [00:01<00:00, 303.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: loss=0.0100\n",
      "F1-macro=0.0186 | F1-micro=0.6033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 392/392 [00:01<00:00, 305.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: loss=0.0100\n",
      "F1-macro=0.0190 | F1-micro=0.6005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 392/392 [00:01<00:00, 299.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: loss=0.0099\n",
      "F1-macro=0.0196 | F1-micro=0.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 392/392 [00:01<00:00, 302.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: loss=0.0099\n",
      "F1-macro=0.0208 | F1-micro=0.6077\n",
      "\n",
      "Final Test:\n",
      "{'f1_macro': 0.021519004340856133, 'f1_micro': 0.6079575054905766}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === MLP Classifier ===\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Init ===\n",
    "input_dim = X_train.size(1)\n",
    "num_classes = y_train.size(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLPClassifier(input_dim, num_classes).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "# === Training ===\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}\")\n",
    "\n",
    "    # === Validation ===\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    print(f\"F1-macro={val_metrics['f1_macro']:.4f} | F1-micro={val_metrics['f1_micro']:.4f}\")\n",
    "\n",
    "# === Test ===\n",
    "print(\"\\nFinal Test:\")\n",
    "test_metrics = evaluate(model, test_loader, device)\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2cb09f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 308/308 [00:01<00:00, 229.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19658 samples generated.\n",
      "Submission file saved: Submission/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv, os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "OUTPUT_PATH = \"Submission/submission.csv\"\n",
    "os.makedirs(\"Submission\", exist_ok=True)\n",
    "\n",
    "all_pids, all_pred_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(X_test), 64), desc=\"Generating predictions\"):\n",
    "        end = start + 64\n",
    "        batch = X_test[start:end]\n",
    "        batch_pids = test_ids[start:end]\n",
    "\n",
    "        logits = model(batch)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        for pid, prob in zip(batch_pids, probs):\n",
    "            pred_row = (prob > THRESHOLD).astype(int)\n",
    "\n",
    "            if pred_row.sum() == 0:\n",
    "                pred_row[prob.argmax()] = 1\n",
    "\n",
    "            labels = [str(j) for j, v in enumerate(pred_row) if v == 1]\n",
    "\n",
    "            all_pids.append(pid)\n",
    "            all_pred_labels.append(labels)\n",
    "\n",
    "print(f\"{len(all_pids)} samples generated.\")\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"label\"])\n",
    "    for pid, labels in zip(all_pids, all_pred_labels):\n",
    "        writer.writerow([pid, \",\".join(labels)])\n",
    "\n",
    "print(f\"Submission file saved: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53f4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1843/1843 [09:47<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 1652/1843 [09:32<01:02,  3.08it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ============================================\n",
    "# 1. DATASET AVEC TEXTE BRUT (pas embeddings)\n",
    "# ============================================\n",
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset qui prend le TEXTE brut, pas les embeddings\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize le texte\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# ============================================\n",
    "# 2. MODÈLE : BERT + Classifier Head\n",
    "# ============================================\n",
    "class BERTMultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture du papier :\n",
    "    - BERT encoder (frozen ou fine-tuné)\n",
    "    - Classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilroberta-base', num_labels=531, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder BERT\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encode avec BERT\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Prend le [CLS] token (première position)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# ============================================\n",
    "# 3. TRAINING LOOP\n",
    "# ============================================\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ============================================\n",
    "# 4. EVALUATION\n",
    "# ============================================\n",
    "def evaluate(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            # Prédictions\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs > threshold).astype(int)\n",
    "            \n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Calcul F1\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    return {'f1_macro': f1_macro, 'f1_micro': f1_micro}\n",
    "\n",
    "# ============================================\n",
    "# 5. MAIN PIPELINE\n",
    "# ============================================\n",
    "def main():\n",
    "    # Configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Charger les données\n",
    "    with open('Amazon_products/train/train_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "        train_texts = [line.strip().split('\\t')[1] for line in f]\n",
    "    \n",
    "    with open('Silver/silver_train_roberta.json', 'r') as f:\n",
    "        silver_labels = json.load(f)\n",
    "    \n",
    "    # Préparer les labels\n",
    "    train_labels = []\n",
    "    for pid in silver_labels.keys():\n",
    "        labels_binary = np.zeros(531)\n",
    "        for label in silver_labels[pid]:\n",
    "            labels_binary[int(label)] = 1\n",
    "        train_labels.append(labels_binary)\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = TextClassificationDataset(\n",
    "        train_texts, \n",
    "        train_labels, \n",
    "        tokenizer\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Modèle\n",
    "    model = BERTMultiLabelClassifier().to(device)\n",
    "    \n",
    "    # Optimizer et loss\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(5):\n",
    "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    # Sauvegarder\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
