AWS Ressources -> 90%

Build a classifier

bash

tasks : Silver label generation (taxonomy + keywords)
Transformer-based classifier training
Optional self-training and hierarchy refinement
Final Kaggle prediction file generation


class ReviewDataset(Dataset):
    def __init__(self, pid2labelids, pid2text, tokenizer):
        self.pids = list(pid2labelids.keys())
        self.pid2labelids = pid2labelids
        self.pid2text = pid2text
        self.tokenizer = tokenizer
    
    def __len__(self):
        return len(self.pids)
    
    def __getitem__(self, idx):
        pid = self.pids[idx]
        text = self.pid2text[pid]
        labels = self.pid2labelids[pid]
        
        encoding = self.tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')
        
        y = torch.zeros(531)
        for label_id in labels:
            y[label_id] = 1.0
        
        return {
            "X": encoding['input_ids'].squeeze(0),
            "attention_mask": encoding['attention_mask'].squeeze(0),
            "y": y
        }

from sklearn.metrics import f1_score

def evaluate(model, dataloader, device="cpu"):
    model.eval()
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for batch in dataloader:
            X = batch["X"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            y = batch["y"].to(device)
            
            # Forward
            logits = model(X, attention_mask)
            
            # Multi-label: sigmoid + threshold
            preds = (torch.sigmoid(logits) > 0.5).float()
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    
    # Sklearn f1_score for multi-label
    f1_macro = f1_score(all_labels, all_preds, average="macro", zero_division=0)
    f1_micro = f1_score(all_labels, all_preds, average="micro", zero_division=0)
    
    return {
        "f1_macro": f1_macro,
        "f1_micro": f1_micro
    }

from sklearn.model_selection import train_test_split
from transformers import RobertaModel, RobertaTokenizer, pipeline, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('distilroberta-base', use_fast=True)

# Split 80/20
train_pids, val_pids = train_test_split(
    list(pid2labelids_silver.keys()), 
    test_size=0.2, 
    random_state=42
)

train_labels = {pid: pid2labelids_silver[pid] for pid in train_pids}
val_labels = {pid: pid2labelids_silver[pid] for pid in val_pids}

print(f"Train: {len(train_labels)}, Val: {len(val_labels)}")

# Datasets
train_dataset = ReviewDataset(train_labels, id2text_train, tokenizer)
val_dataset = ReviewDataset(val_labels, id2text_train, tokenizer)
test_dataset = ReviewDataset(pid2labelids_test, id2text_test, tokenizer)

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=16, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=16, num_workers=4, pin_memory=True)

import torch
import torch.nn as nn
from torch.optim import AdamW
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup

# === Config ===
MODEL_NAME = "distilroberta-base"
NUM_EPOCHS = 5
LR = 2e-5
BATCH_SIZE = 16
WARMUP_RATIO = 0.1
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("Device:", device)

# === Model ===
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_CLASSES,
    problem_type="multi_label_classification"
).to(device)

# === Optimizer + Scheduler ===
optimizer = AdamW(model.parameters(), lr=LR)
num_training_steps = len(train_loader) * NUM_EPOCHS
num_warmup_steps = int(WARMUP_RATIO * num_training_steps)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

# === Loss function ===
criterion = nn.BCEWithLogitsLoss()

# === Training loop ===
best_f1 = 0.0

for epoch in range(NUM_EPOCHS):
    model.train()
    total_loss = 0.0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}"):
        X = batch["X"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        y = batch["y"].to(device).float()

        optimizer.zero_grad()
        outputs = model(X, attention_mask=attention_mask)
        logits = outputs.logits
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"\nEpoch {epoch+1} | Training loss: {avg_loss:.4f}")

    # === Validation ===
    val_metrics = evaluate(model, val_loader, device)
    f1_macro = val_metrics["f1_macro"]
    f1_micro = val_metrics["f1_micro"]
    print(f"Validation F1-macro: {f1_macro:.4f} | F1-micro: {f1_micro:.4f}")

    if f1_macro > best_f1:
        best_f1 = f1_macro
        torch.save(model.state_dict(), "Model/best_roberta_finetuned.pt")
        print("New best model saved!")

print(f"\nBest validation F1-macro: {best_f1:.4f}")

# === Final test ===
print("\n=== Final Test ===")
test_metrics = evaluate(model, test_loader, device)
print(test_metrics)

best_f1 = 0.0
best_state = copy.deepcopy(teacher.state_dict())
wait = 0
patience = 5
current_labels = dict(pid2labelids_silver)

for epoch in range(1, EPOCHS + 1):
    print(f"\n=== Epoch {epoch}/{EPOCHS} ===")

    # Pseudo-label update all pseudo_update_freq epochs
    if epoch % pseudo_update_freq == 1 and epoch > 1:
        progress = min(1.0, epoch / EPOCHS)
        thr = pseudo_threshold_start - (pseudo_threshold_start - pseudo_threshold_end) * progress
        new_pseudo = generate_pseudo_labels(thr)
        if new_pseudo:
            old = len(current_labels)
            current_labels.update(new_pseudo)
            print(f"Added {len(current_labels) - old} pseudo-labeled samples (threshold={thr:.2f})")

    # Dynamic dataset based on current labels
    train_ds = ReviewDataset(current_labels, id2text_train, tokenizer)
    train_ld = DataLoader(train_ds, batch_size=64, shuffle=True)

    student.train()
    teacher.eval()
    total_loss = 0.0

    for batch in tqdm(train_ld, desc=f"Train epoch {epoch}", leave=False):
        X = batch["X"].to(device)
        attn = batch["attention_mask"].to(device)
        y = batch["y"].to(device).float()

        # Student predictions (with a small noise for stability)
        noise = torch.randn_like(X) * 0.02
        logits_s = student(X + noise, attention_mask=attn).logits

        with torch.no_grad():
            logits_t = teacher(X, attention_mask=attn).logits

        loss_sup = criterion(logits_s, y)
        loss_cons = consistency_loss(logits_s, logits_t)
        lam_dyn = lambda_cons * min(1.0, epoch / 10)
        loss = loss_sup + lam_dyn * loss_cons

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        ema_update(teacher, student, alpha_ema)

        total_loss += loss.item()

    scheduler.step()
    val_metrics = evaluate(teacher, val_loader, device)
    f1 = val_metrics["f1_macro"]
    print(f"Epoch {epoch} | loss={total_loss/len(train_ld):.4f} | val F1={f1:.4f}")

    if f1 > best_f1:
        best_f1 = f1
        best_state = copy.deepcopy(teacher.state_dict())
        torch.save(best_state, "Model/roberta_selftrained.pt")
        print(f"New best F1={best_f1:.4f}")
        wait = 0
    else:
        wait += 1
        if wait >= patience:
            print("Early stopping.")
            break

# Load the best teacher
teacher.load_state_dict(best_state)
teacher.eval()
final_test = evaluate(teacher, test_loader, device)
print("\n=== FINAL TEST (Self-Trained RoBERTa) ===")
print(final_test)

import copy
import torch
import torch.nn.functional as F
from transformers import RobertaForSequenceClassification
from tqdm import tqdm

# === Config ===
EPOCHS = 20
pseudo_update_freq = 3
pseudo_threshold_start = 0.80
pseudo_threshold_end = 0.95
alpha_ema = 0.999     # EMA teacher smoothing
lambda_cons = 1.5     # consistency loss strength

# Load finetune
student = RobertaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_CLASSES,
    problem_type="multi_label_classification"
).to(device)
student.load_state_dict(torch.load("Model/best_roberta_finetuned.pt", map_location=device))

teacher = copy.deepcopy(student).to(device)
teacher.eval()

optimizer = torch.optim.AdamW(student.parameters(), lr=2e-5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
criterion = nn.BCEWithLogitsLoss()

# TP3
def ema_update(teacher, student, alpha=0.999):
    for t_param, s_param in zip(teacher.parameters(), student.parameters()):
        t_param.data = alpha * t_param.data + (1.0 - alpha) * s_param.data


def consistency_loss(logits_s, logits_t):
    # KL-divergence entre prÃ©dictions student/teacher
    ps = F.log_softmax(logits_s, dim=1)
    pt = F.softmax(logits_t / 0.5, dim=1)
    return F.kl_div(ps, pt, reduction="batchmean")


def generate_pseudo_labels(threshold):
    teacher.eval()
    pseudo_labels = {}
    with torch.no_grad():
        for batch in tqdm(train_loader, desc=f"Generating pseudo-labels @thr={threshold:.2f}", leave=False):
            X = batch["X"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            logits = teacher(X, attention_mask=attention_mask).logits
            probs = torch.sigmoid(logits).cpu().numpy()
            for i, pid in enumerate(batch["pid"]):
                confident = np.where(probs[i] > threshold)[0].tolist()
                if confident:
                    pseudo_labels[pid] = confident
    return pseudo_labels