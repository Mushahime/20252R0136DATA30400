{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b05cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "# Seed pour reproductibilité\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0700e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\"Amazon_products\")\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "\n",
    "TEST_CORPUS_PATH = TEST_DIR / \"test_corpus.txt\"\n",
    "TRAIN_CORPUS_PATH = TRAIN_DIR / \"train_corpus.txt\"\n",
    "\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\"\n",
    "CLASS_RELATED_PATH = ROOT / \"class_related_keywords.txt\"\n",
    "CLASS_PATH = ROOT / \"classes.txt\"\n",
    "\n",
    "SUBMISSION_PATH = \"Submission/submission.csv\"\n",
    "\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9a2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(path):\n",
    "    \"\"\"Load corpus into {id: text} dictionary.\"\"\"\n",
    "    id2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                id, text = parts\n",
    "                id2text[id] = text\n",
    "    return id2text\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"Load multi-label data into {id: [labels]} dictionary.\"\"\"\n",
    "    id2labels = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, label = parts\n",
    "                pid = int(pid)\n",
    "                label = int(label)\n",
    "                if pid not in id2labels:\n",
    "                    id2labels[pid] = []\n",
    "                id2labels[pid].append(label)\n",
    "    return id2labels\n",
    "\n",
    "def load_class_keywords(path):\n",
    "    \"\"\"Load class keywords into {class_name: [keywords]} dictionary.\"\"\"\n",
    "    class2keywords = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            classname, keywords = line.strip().split(\":\", 1)\n",
    "            keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
    "            class2keywords[classname] = keyword_list\n",
    "    return class2keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6148cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 29487 samples\n",
      "Test: 19658 samples\n",
      "Classes: 531\n"
     ]
    }
   ],
   "source": [
    "id2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "id2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "\n",
    "# Classes\n",
    "id2class = load_corpus(CLASS_PATH)\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "class2related = load_class_keywords(CLASS_RELATED_PATH)\n",
    "\n",
    "# Silver labels (RoBERTa - les meilleurs)\n",
    "with open(\"Silver/silver_train_roberta.json\", \"r\") as f:\n",
    "    pid2labelids_silver = json.load(f)\n",
    "    \n",
    "\n",
    "print(f\"Train: {len(id2text_train)} samples\")\n",
    "print(f\"Test: {len(id2text_test)} samples\")\n",
    "print(f\"Classes: {len(id2class)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d93d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings: torch.Size([29487, 768])\n",
      "Test embeddings: torch.Size([19658, 768])\n",
      "Label embeddings: torch.Size([531, 768])\n",
      "Input dimension: 768\n",
      "Num classes: 531\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "X_train = torch.load(\"Embeddings/X_train.pt\").to(device)\n",
    "X_test = torch.load(\"Embeddings/X_test.pt\").to(device)\n",
    "label_emb = torch.load(\"Embeddings/label_emb.pt\").to(device)\n",
    "test_ids = list(id2text_test.keys())\n",
    "train_ids = list(id2text_train.keys())\n",
    "\n",
    "print(f\"Train embeddings: {X_train.shape}\")\n",
    "print(f\"Test embeddings: {X_test.shape}\")\n",
    "print(f\"Label embeddings: {label_emb.shape}\")\n",
    "\n",
    "# Index mapping\n",
    "pid2idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "input_dim = X_train.size(1)\n",
    "num_classes = NUM_CLASSES\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Num classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c24d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductCategoryDataset(Dataset):\n",
    "    \"\"\"Dataset using pre-calculated embeddings (train or test compatible)\"\"\"\n",
    "    def __init__(self, pid2label, pid2idx, embeddings, num_classes=531):\n",
    "        self.pid2label = pid2label\n",
    "        self.pid2idx = pid2idx\n",
    "        self.embeddings = embeddings\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if self.pid2label is not None:\n",
    "            self.pids = list(pid2label.keys())\n",
    "            self.has_labels = True\n",
    "        else:\n",
    "            self.pids = list(pid2idx.keys())\n",
    "            self.has_labels = False\n",
    "\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[self.indices[idx]]\n",
    "\n",
    "        if self.has_labels:\n",
    "            y = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "            for label in self.pid2label[self.pids[idx]]:\n",
    "                y[label] = 1.0\n",
    "            return {\"X\": emb, \"y\": y}\n",
    "        else:\n",
    "            return {\"X\": emb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5fc869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device=\"cpu\", threshold=0.5):\n",
    "    \"\"\"Évalue le modèle sur un dataloader\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].cpu().numpy()\n",
    "            \n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs > threshold).astype(int)\n",
    "            \n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y)\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    \n",
    "    return {\"f1_macro\": f1_macro, \"f1_micro\": f1_micro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e6e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"Simple MLP for classification multi-label\"\"\"\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91687a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 23589 | Val: 5898\n"
     ]
    }
   ],
   "source": [
    "# Split train/val\n",
    "silver_pids = list(pid2labelids_silver.keys())\n",
    "train_pids, val_pids = train_test_split(silver_pids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_labels = {pid: pid2labelids_silver[pid] for pid in train_pids}\n",
    "val_labels = {pid: pid2labelids_silver[pid] for pid in val_pids}\n",
    "\n",
    "print(f\"Train: {len(train_labels)} | Val: {len(val_labels)}\")\n",
    "\n",
    "# Datasets\n",
    "train_dataset = ProductCategoryDataset(train_labels, pid2idx, X_train, num_classes=NUM_CLASSES)\n",
    "val_dataset = ProductCategoryDataset(val_labels, pid2idx, X_train, num_classes=NUM_CLASSES)\n",
    "\n",
    "test_dataset = ProductCategoryDataset(\n",
    "    None,\n",
    "    {pid: i for i, pid in enumerate(test_ids)},\n",
    "    X_test,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789f6fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1097 (sup: 0.0847, cons: 0.0499)\n",
      "Val F1-macro=0.0013 | F1-micro=0.2609\n",
      "New best model (F1-macro=0.0013)\n",
      "\n",
      "=== Epoch 2/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0173 (sup: 0.0172, cons: 0.0002)\n",
      "Val F1-macro=0.0014 | F1-micro=0.2490\n",
      "New best model (F1-macro=0.0014)\n",
      "\n",
      "=== Epoch 3/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0165 (sup: 0.0164, cons: 0.0001)\n",
      "Val F1-macro=0.0017 | F1-micro=0.2518\n",
      "New best model (F1-macro=0.0017)\n",
      "\n",
      "=== Epoch 4/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0159 (sup: 0.0159, cons: 0.0001)\n",
      "Val F1-macro=0.0026 | F1-micro=0.3057\n",
      "New best model (F1-macro=0.0026)\n",
      "\n",
      "=== Epoch 5/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0154 (sup: 0.0153, cons: 0.0001)\n",
      "Val F1-macro=0.0033 | F1-micro=0.3470\n",
      "New best model (F1-macro=0.0033)\n",
      "\n",
      "=== Epoch 6/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0149 (sup: 0.0149, cons: 0.0001)\n",
      "Val F1-macro=0.0042 | F1-micro=0.3867\n",
      "New best model (F1-macro=0.0042)\n",
      "\n",
      "=== Epoch 7/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0145 (sup: 0.0145, cons: 0.0001)\n",
      "Val F1-macro=0.0052 | F1-micro=0.4249\n",
      "New best model (F1-macro=0.0052)\n",
      "\n",
      "=== Epoch 8/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0141 (sup: 0.0141, cons: 0.0001)\n",
      "Val F1-macro=0.0059 | F1-micro=0.4569\n",
      "New best model (F1-macro=0.0059)\n",
      "\n",
      "=== Epoch 9/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0137 (sup: 0.0137, cons: 0.0001)\n",
      "Val F1-macro=0.0064 | F1-micro=0.4784\n",
      "New best model (F1-macro=0.0064)\n",
      "\n",
      "=== Epoch 10/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0135 (sup: 0.0134, cons: 0.0001)\n",
      "Val F1-macro=0.0068 | F1-micro=0.4930\n",
      "New best model (F1-macro=0.0068)\n",
      "\n",
      "=== Epoch 11/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0132 (sup: 0.0132, cons: 0.0001)\n",
      "Val F1-macro=0.0073 | F1-micro=0.5034\n",
      "New best model (F1-macro=0.0073)\n",
      "\n",
      "=== Epoch 12/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0131 (sup: 0.0130, cons: 0.0001)\n",
      "Val F1-macro=0.0075 | F1-micro=0.5088\n",
      "New best model (F1-macro=0.0075)\n",
      "\n",
      "=== Epoch 13/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0129 (sup: 0.0129, cons: 0.0001)\n",
      "Val F1-macro=0.0078 | F1-micro=0.5139\n",
      "New best model (F1-macro=0.0078)\n",
      "\n",
      "=== Epoch 14/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0128 (sup: 0.0128, cons: 0.0001)\n",
      "Val F1-macro=0.0079 | F1-micro=0.5179\n",
      "New best model (F1-macro=0.0079)\n",
      "\n",
      "=== Epoch 15/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0127 (sup: 0.0127, cons: 0.0001)\n",
      "Val F1-macro=0.0080 | F1-micro=0.5198\n",
      "New best model (F1-macro=0.0080)\n",
      "\n",
      "=== Epoch 16/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0126 (sup: 0.0126, cons: 0.0001)\n",
      "Val F1-macro=0.0081 | F1-micro=0.5216\n",
      "New best model (F1-macro=0.0081)\n",
      "\n",
      "=== Epoch 17/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0126 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0082 | F1-micro=0.5230\n",
      "New best model (F1-macro=0.0082)\n",
      "\n",
      "=== Epoch 18/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0126 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0082 | F1-micro=0.5244\n",
      "New best model (F1-macro=0.0082)\n",
      "\n",
      "=== Epoch 19/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0082 | F1-micro=0.5244\n",
      "No improvement: 1/5\n",
      "\n",
      "=== Epoch 20/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0082 | F1-micro=0.5242\n",
      "New best model (F1-macro=0.0082)\n",
      "\n",
      "=== Epoch 21/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0082 | F1-micro=0.5243\n",
      "No improvement: 1/5\n",
      "\n",
      "=== Epoch 22/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0082 | F1-micro=0.5243\n",
      "No improvement: 2/5\n",
      "\n",
      "=== Epoch 23/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0083 | F1-micro=0.5246\n",
      "New best model (F1-macro=0.0083)\n",
      "\n",
      "=== Epoch 24/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125 (sup: 0.0125, cons: 0.0001)\n",
      "Val F1-macro=0.0083 | F1-micro=0.5249\n",
      "New best model (F1-macro=0.0083)\n",
      "\n",
      "=== Epoch 25/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125 (sup: 0.0124, cons: 0.0001)\n",
      "Val F1-macro=0.0083 | F1-micro=0.5257\n",
      "New best model (F1-macro=0.0083)\n",
      "\n",
      "=== Epoch 26/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0124 (sup: 0.0124, cons: 0.0001)\n",
      "Val F1-macro=0.0085 | F1-micro=0.5271\n",
      "New best model (F1-macro=0.0085)\n",
      "\n",
      "=== Epoch 27/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0124 (sup: 0.0123, cons: 0.0001)\n",
      "Val F1-macro=0.0086 | F1-micro=0.5290\n",
      "New best model (F1-macro=0.0086)\n",
      "\n",
      "=== Epoch 28/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0123 (sup: 0.0122, cons: 0.0001)\n",
      "Val F1-macro=0.0087 | F1-micro=0.5312\n",
      "New best model (F1-macro=0.0087)\n",
      "\n",
      "=== Epoch 29/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0122 (sup: 0.0121, cons: 0.0001)\n",
      "Val F1-macro=0.0088 | F1-micro=0.5343\n",
      "New best model (F1-macro=0.0088)\n",
      "\n",
      "=== Epoch 30/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0120 (sup: 0.0120, cons: 0.0001)\n",
      "Val F1-macro=0.0090 | F1-micro=0.5374\n",
      "New best model (F1-macro=0.0090)\n",
      "\n",
      "=== Epoch 31/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0119 (sup: 0.0119, cons: 0.0001)\n",
      "Val F1-macro=0.0094 | F1-micro=0.5417\n",
      "New best model (F1-macro=0.0094)\n",
      "\n",
      "=== Epoch 32/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0117 (sup: 0.0117, cons: 0.0001)\n",
      "Val F1-macro=0.0098 | F1-micro=0.5460\n",
      "New best model (F1-macro=0.0098)\n",
      "\n",
      "=== Epoch 33/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0116 (sup: 0.0115, cons: 0.0001)\n",
      "Val F1-macro=0.0102 | F1-micro=0.5509\n",
      "New best model (F1-macro=0.0102)\n",
      "\n",
      "=== Epoch 34/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0114 (sup: 0.0114, cons: 0.0001)\n",
      "Val F1-macro=0.0106 | F1-micro=0.5565\n",
      "New best model (F1-macro=0.0106)\n",
      "\n",
      "=== Epoch 35/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0112 (sup: 0.0112, cons: 0.0001)\n",
      "Val F1-macro=0.0111 | F1-micro=0.5613\n",
      "New best model (F1-macro=0.0111)\n",
      "\n",
      "=== Epoch 36/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0111 (sup: 0.0111, cons: 0.0001)\n",
      "Val F1-macro=0.0116 | F1-micro=0.5662\n",
      "New best model (F1-macro=0.0116)\n",
      "\n",
      "=== Epoch 37/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0109 (sup: 0.0109, cons: 0.0001)\n",
      "Val F1-macro=0.0122 | F1-micro=0.5715\n",
      "New best model (F1-macro=0.0122)\n",
      "\n",
      "=== Epoch 38/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0108 (sup: 0.0108, cons: 0.0001)\n",
      "Val F1-macro=0.0127 | F1-micro=0.5757\n",
      "New best model (F1-macro=0.0127)\n",
      "\n",
      "=== Epoch 39/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0106 (sup: 0.0106, cons: 0.0001)\n",
      "Val F1-macro=0.0132 | F1-micro=0.5800\n",
      "New best model (F1-macro=0.0132)\n",
      "\n",
      "=== Epoch 40/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0105 (sup: 0.0105, cons: 0.0001)\n",
      "Val F1-macro=0.0137 | F1-micro=0.5843\n",
      "New best model (F1-macro=0.0137)\n",
      "\n",
      "=== Epoch 41/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0104 (sup: 0.0104, cons: 0.0001)\n",
      "Val F1-macro=0.0141 | F1-micro=0.5873\n",
      "New best model (F1-macro=0.0141)\n",
      "\n",
      "=== Epoch 42/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0103 (sup: 0.0103, cons: 0.0001)\n",
      "Val F1-macro=0.0147 | F1-micro=0.5913\n",
      "New best model (F1-macro=0.0147)\n",
      "\n",
      "=== Epoch 43/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0102 (sup: 0.0102, cons: 0.0001)\n",
      "Val F1-macro=0.0153 | F1-micro=0.5947\n",
      "New best model (F1-macro=0.0153)\n",
      "\n",
      "=== Epoch 44/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0101 (sup: 0.0101, cons: 0.0001)\n",
      "Val F1-macro=0.0159 | F1-micro=0.5973\n",
      "New best model (F1-macro=0.0159)\n",
      "\n",
      "=== Epoch 45/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0100 (sup: 0.0100, cons: 0.0001)\n",
      "Val F1-macro=0.0165 | F1-micro=0.6007\n",
      "New best model (F1-macro=0.0165)\n",
      "\n",
      "=== Epoch 46/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0100 (sup: 0.0099, cons: 0.0001)\n",
      "Val F1-macro=0.0169 | F1-micro=0.6032\n",
      "New best model (F1-macro=0.0169)\n",
      "\n",
      "=== Epoch 47/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0099 (sup: 0.0099, cons: 0.0001)\n",
      "Val F1-macro=0.0173 | F1-micro=0.6049\n",
      "New best model (F1-macro=0.0173)\n",
      "\n",
      "=== Epoch 48/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0098 (sup: 0.0098, cons: 0.0001)\n",
      "Val F1-macro=0.0176 | F1-micro=0.6069\n",
      "New best model (F1-macro=0.0176)\n",
      "\n",
      "=== Epoch 49/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0098 (sup: 0.0098, cons: 0.0001)\n",
      "Val F1-macro=0.0176 | F1-micro=0.6079\n",
      "New best model (F1-macro=0.0176)\n",
      "\n",
      "=== Epoch 50/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0098 (sup: 0.0097, cons: 0.0001)\n",
      "Val F1-macro=0.0179 | F1-micro=0.6083\n",
      "New best model (F1-macro=0.0179)\n",
      "\n",
      "==================================================\n",
      "FINAL EVALUATION\n",
      "==================================================\n",
      "Best Val F1-macro: 0.0179\n",
      "Best Val F1-micro: 0.6083\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Init Models ===\n",
    "student = MLPClassifier(input_dim, num_classes).to(device)\n",
    "teacher = copy.deepcopy(student).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=2e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# === Self-training parameters ===\n",
    "alpha_ema = 0.995                    # EMA update factor\n",
    "lambda_cons = 0.5                    # weight for consistency loss\n",
    "pseudo_update_freq = 3               # update pseudo-labels every N epochs\n",
    "pseudo_threshold = 0.9               # threshold for teacher pseudo-labels\n",
    "patience = 5\n",
    "EPOCHS = 50\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_model = copy.deepcopy(student.state_dict())\n",
    "wait = 0\n",
    "\n",
    "def ema_update(teacher, student, alpha):\n",
    "    \"\"\"Exponential Moving Average update for teacher\"\"\"\n",
    "    for t_param, s_param in zip(teacher.parameters(), student.parameters()):\n",
    "        t_param.data = alpha * t_param.data + (1 - alpha) * s_param.data\n",
    "\n",
    "def consistency_loss(logits_s, logits_t):\n",
    "    \"\"\"MSE between student and teacher predictions\"\"\"\n",
    "    return F.mse_loss(torch.sigmoid(logits_s), torch.sigmoid(logits_t))\n",
    "\n",
    "def generate_pseudo_labels(model, loader, threshold=0.7):\n",
    "    \"\"\"Generate high-confidence pseudo-labels from teacher\"\"\"\n",
    "    model.eval()\n",
    "    pseudo_dict = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Generating pseudo-labels\", leave=False):\n",
    "            X = batch[\"X\"].to(device)\n",
    "            \n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            # For each sample in batch\n",
    "            for i, prob in enumerate(probs):\n",
    "                # Only keep high-confidence predictions\n",
    "                confident_labels = [j for j, p in enumerate(prob) if p > threshold]\n",
    "                \n",
    "                # Only add if we have 2-3 labels (reasonable for this task)\n",
    "                if 2 <= len(confident_labels) <= 3:\n",
    "                    pass \n",
    "    \n",
    "    return pseudo_dict\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n",
    "\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    total_loss = 0.0\n",
    "    total_sup = 0.0\n",
    "    total_cons = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch}\", leave=False):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        # --- Student forward ---\n",
    "        logits_s = student(X)\n",
    "        \n",
    "        # --- Teacher forward (for consistency) ---\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(X)\n",
    "\n",
    "        # --- Loss ---\n",
    "        loss_sup = criterion(logits_s, y)\n",
    "        loss_cons = consistency_loss(logits_s, logits_t)\n",
    "        loss = loss_sup + lambda_cons * loss_cons\n",
    "\n",
    "        # --- Backprop ---\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Update teacher with EMA ---\n",
    "        ema_update(teacher, student, alpha_ema)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_sup += loss_sup.item()\n",
    "        total_cons += loss_cons.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # --- Print training stats ---\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_sup = total_sup / len(train_loader)\n",
    "    avg_cons = total_cons / len(train_loader)\n",
    "    \n",
    "    print(f\"Loss: {avg_loss:.4f} (sup: {avg_sup:.4f}, cons: {avg_cons:.4f})\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    val_metrics = evaluate(teacher, val_loader, device)\n",
    "    print(f\"Val F1-macro={val_metrics['f1_macro']:.4f} | \"\n",
    "          f\"F1-micro={val_metrics['f1_micro']:.4f}\")\n",
    "\n",
    "    # --- Save best model ---\n",
    "    if val_metrics['f1_macro'] > best_f1:\n",
    "        best_f1 = val_metrics['f1_macro']\n",
    "        best_model = copy.deepcopy(teacher.state_dict())\n",
    "        print(f\"New best model (F1-macro={best_f1:.4f})\")\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        print(f\"No improvement: {wait}/{patience}\")\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# === Load best model ===\n",
    "teacher.load_state_dict(best_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test on validation set (since we don't have true test labels)\n",
    "test_result = evaluate(teacher, val_loader, device)\n",
    "print(f\"Best Val F1-macro: {test_result['f1_macro']:.4f}\")\n",
    "print(f\"Best Val F1-micro: {test_result['f1_micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a35cc4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model saved to Models/mlp_selftraining_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "torch.save(teacher.state_dict(), \"Model/mlp_selftraining_best.pt\")\n",
    "print(\"\\nBest model saved to Models/mlp_selftraining_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5efa1796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 308/308 [00:01<00:00, 237.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19658 samples generated.\n",
      "Submission file saved: Submission/MLP.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv, os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "OUTPUT_PATH = \"Submission/MLP.csv\"\n",
    "os.makedirs(\"Submission\", exist_ok=True)\n",
    "\n",
    "all_pids, all_pred_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(X_test), 64), desc=\"Generating predictions\"):\n",
    "        end = start + 64\n",
    "        batch = X_test[start:end]\n",
    "        batch_pids = test_ids[start:end]\n",
    "\n",
    "        logits = teacher(batch)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        for pid, prob in zip(batch_pids, probs):\n",
    "            pred_row = (prob > THRESHOLD).astype(int)\n",
    "\n",
    "            if pred_row.sum() == 0:\n",
    "                pred_row[prob.argmax()] = 1\n",
    "\n",
    "            labels = [str(j) for j, v in enumerate(pred_row) if v == 1]\n",
    "\n",
    "            all_pids.append(pid)\n",
    "            all_pred_labels.append(labels)\n",
    "\n",
    "print(f\"{len(all_pids)} samples generated.\")\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"label\"])\n",
    "    for pid, labels in zip(all_pids, all_pred_labels):\n",
    "        writer.writerow([pid, \",\".join(labels)])\n",
    "\n",
    "print(f\"Submission file saved: {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
