{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a128149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# --- Default paths ---\n",
    "ROOT = Path(\"Amazon_products\")                 # Root directory\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "CLASS_PATH = ROOT / \"classes.txt\"\n",
    "\n",
    "SILVER_DIR = Path(\"Silver\")\n",
    "MODEL_PATH = \"Embeddings/roberta_student.pt\"             # Fine-tuned model path\n",
    "OUTPUT_PATH = SILVER_DIR / \"pseudo_labels_roberta.json\"\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531\n",
    "MAX_LABELS = 3\n",
    "CONFIDENCE_THRESHOLD = 0.65\n",
    "CONFIDENCE_MIN = 0.55\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795319ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 29487\n",
      "Number of classes: 531\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(path):\n",
    "    \"\"\"Load corpus into {id: text} dictionary.\"\"\"\n",
    "    id2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                id2text[int(pid)] = text\n",
    "    return id2text\n",
    "\n",
    "# Load training data\n",
    "TRAIN_CORPUS_PATH = TRAIN_DIR / \"train_corpus.txt\"\n",
    "id2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "\n",
    "# Load label names\n",
    "id2label = {}\n",
    "with open(CLASS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            cid, cname = parts\n",
    "            id2label[int(cid)] = cname.strip()\n",
    "\n",
    "print(f\"Train samples: {len(id2text_train)}\")\n",
    "print(f\"Number of classes: {len(id2label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bea3d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleRobertaClassifier(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=531, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class SimpleRobertaClassifier(torch.nn.Module):\n",
    "    \"\"\"A small RoBERTa classifier for multi-label prediction.\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = outputs.last_hidden_state[:, 0, :]   # CLS token representation\n",
    "        x = self.dropout(cls)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Load tokenizer + model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = SimpleRobertaClassifier(NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ee9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 29487/29487 [15:29<00:00, 31.71it/s]\n"
     ]
    }
   ],
   "source": [
    "pseudo_labels = {}\n",
    "pseudo_scores = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pid, text in tqdm(id2text_train.items(), desc=\"Predicting\"):\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits[0]).cpu().numpy()\n",
    "\n",
    "        # sort by descending confidence\n",
    "        top_idx = probs.argsort()[::-1]\n",
    "        selected_labels, selected_scores = [], []\n",
    "\n",
    "        for idx in top_idx:\n",
    "            if len(selected_labels) >= MAX_LABELS:\n",
    "                break\n",
    "            if probs[idx] > CONFIDENCE_THRESHOLD:\n",
    "                selected_labels.append(int(idx))\n",
    "                selected_scores.append(float(probs[idx]))\n",
    "\n",
    "        if len(selected_labels) > 0:\n",
    "            pseudo_labels[pid] = selected_labels\n",
    "            pseudo_scores[pid] = selected_scores\n",
    "\n",
    "# Filter by minimum top-1 confidence\n",
    "pid2labelids_pseudo_filtered = {\n",
    "    pid: labels\n",
    "    for pid, labels in pseudo_labels.items()\n",
    "    if pseudo_scores[pid][0] > CONFIDENCE_MIN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e353b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics\n",
      "Total products: 29487\n",
      "With pseudo-labels (conf > 0.65): 22547\n",
      "High-confidence (top-1 > 0.55): 22547\n",
      "Unlabeled: 6940\n",
      "\n",
      "Sample predictions:\n",
      "\n",
      "0: omron hem 790it automatic blood pressure monitor with advanced omron health management software so f...\n",
      "   → 74 | medical_supplies_equipment (score=0.823)\n",
      "   → 23 | health_personal_care (score=0.815)\n",
      "   → 118 | health_monitors (score=0.811)\n",
      "\n",
      "1: natural factors whey factors chocolate works well , but there is a lot of dead space in the containe...\n",
      "   → 265 | candy_chocolate (score=0.748)\n",
      "\n",
      "2: clif bar builder 's bar , 2 . 4 ounce bars i love the peanut butter builder 's bars . while amazon i...\n",
      "   → 271 | snack_food (score=0.752)\n",
      "   → 265 | candy_chocolate (score=0.700)\n",
      "\n",
      "3: andis 1875 watt professional ceramic ionic hair dryer i was a little hesitant to purchase since it w...\n",
      "   → 64 | hair_care (score=0.961)\n",
      "   → 10 | beauty (score=0.812)\n",
      "\n",
      "4: clif bar energy bars these were cheaper than what i had bought at sam 's and worked very well . they...\n",
      "   → 265 | candy_chocolate (score=0.769)\n",
      "   → 271 | snack_food (score=0.688)\n",
      "\n",
      "Saved to Silver\\pseudo_labels_roberta.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStatistics\")\n",
    "print(f\"Total products: {len(id2text_train)}\")\n",
    "print(f\"With pseudo-labels (conf > {CONFIDENCE_THRESHOLD}): {len(pseudo_labels)}\")\n",
    "print(f\"High-confidence (top-1 > {CONFIDENCE_MIN}): {len(pid2labelids_pseudo_filtered)}\")\n",
    "print(f\"Unlabeled: {len(id2text_train) - len(pid2labelids_pseudo_filtered)}\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "for pid in list(pid2labelids_pseudo_filtered.keys())[:5]:\n",
    "    labels = pid2labelids_pseudo_filtered[pid]\n",
    "    scores = pseudo_scores[pid]\n",
    "    print(f\"\\n{pid}: {id2text_train[pid][:100]}...\")\n",
    "    for lab_id, score in zip(labels, scores):\n",
    "        print(f\"   → {lab_id} | {id2label[lab_id]} (score={score:.3f})\")\n",
    "\n",
    "\n",
    "os.makedirs(SILVER_DIR, exist_ok=True)\n",
    "output = {\n",
    "    \"pseudo_labels_all\": pseudo_labels,\n",
    "    \"pseudo_scores\": pseudo_scores,\n",
    "    \"pseudo_labels_filtered\": pid2labelids_pseudo_filtered,\n",
    "    \"stats\": {\n",
    "        \"total\": len(id2text_train),\n",
    "        \"with_pseudo\": len(pseudo_labels),\n",
    "        \"high_conf\": len(pid2labelids_pseudo_filtered),\n",
    "        \"unlabeled\": len(id2text_train) - len(pid2labelids_pseudo_filtered)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ceaf928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Silver labels loaded: 29487 items\n",
      "  → Pseudo labels (confident) loaded: 22547 items\n",
      "\n",
      "Merge complete!\n",
      "Docs merged via common labels : 8750\n",
      "Docs merged via union         : 13797\n",
      "Docs added only from pseudo   : 0\n",
      "Total merged docs             : 22547\n",
      "\n",
      "Stats on merged dataset:\n",
      "  Min labels per doc : 2\n",
      "  Max labels per doc : 3\n",
      "  Avg labels per doc : 2.74\n",
      "\n",
      "Sample merged entries:\n",
      "  Product 0 → labels: [74, 118, 23]\n",
      "  Product 1 → labels: [218, 269, 265]\n",
      "  Product 2 → labels: [265, 271]\n",
      "  Product 3 → labels: [64, 10]\n",
      "  Product 4 → labels: [429, 271, 26]\n",
      "  ...\n",
      "\n",
      "Saved merged dataset to: Silver/train_merged_for_selftraining.json\n",
      "\n",
      "Unlabeled products kept aside: 6940\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# --- Load silver labels ---\n",
    "with open(\"Silver/hier.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    silver_data = json.load(f)\n",
    "silver_labels = silver_data[\"silver_hierarchy\"]\n",
    "print(f\" Silver labels loaded: {len(silver_labels)} items\")\n",
    "\n",
    "# --- Load pseudo labels (only confident ones) ---\n",
    "with open(\"Silver/pseudo_labels_roberta.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pseudo_data = json.load(f)\n",
    "pseudo_labels = pseudo_data[\"pseudo_labels_filtered\"]\n",
    "print(f\"  → Pseudo labels (confident) loaded: {len(pseudo_labels)} items\")\n",
    "pseudo_labels = {int(k): v for k, v in pseudo_labels.items()}\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n",
    "\n",
    "merged = {}\n",
    "count_common, count_union, count_added = 0, 0, 0\n",
    "\n",
    "for pid_str, silver_labs in silver_labels.items():\n",
    "    pid = int(pid_str)\n",
    "\n",
    "    # Only merge if this product has confident pseudo-labels\n",
    "    if pid not in pseudo_labels:\n",
    "        continue  # skip unlabeled products\n",
    "\n",
    "    silver_set = set(silver_labs)\n",
    "    pseudo_set = set(pseudo_labels[pid])\n",
    "\n",
    "    # Intersection (most reliable overlap)\n",
    "    common = list(silver_set & pseudo_set)\n",
    "\n",
    "    if len(common) >= MIN_LABELS:\n",
    "        merged[pid] = common[:MAX_LABELS]\n",
    "        count_common += 1\n",
    "    else:\n",
    "        # Otherwise combine both lists (union) and limit\n",
    "        combined = list(silver_set | pseudo_set)\n",
    "        random.shuffle(combined)\n",
    "        merged[pid] = combined[:MAX_LABELS]\n",
    "        count_union += 1\n",
    "\n",
    "# Also add confident pseudo-only products (not in silver)\n",
    "for pid, labels in pseudo_labels.items():\n",
    "    if str(pid) not in silver_labels:\n",
    "        if len(labels) >= MIN_LABELS:\n",
    "            merged[pid] = labels[:MAX_LABELS]\n",
    "            count_added += 1\n",
    "\n",
    "print(\"\\nMerge complete!\")\n",
    "print(f\"Docs merged via common labels : {count_common}\")\n",
    "print(f\"Docs merged via union         : {count_union}\")\n",
    "print(f\"Docs added only from pseudo   : {count_added}\")\n",
    "print(f\"Total merged docs             : {len(merged)}\")\n",
    "\n",
    "# --- Stats ---\n",
    "lens = [len(v) for v in merged.values()]\n",
    "print(f\"\\nStats on merged dataset:\")\n",
    "print(f\"  Min labels per doc : {min(lens)}\")\n",
    "print(f\"  Max labels per doc : {max(lens)}\")\n",
    "print(f\"  Avg labels per doc : {sum(lens)/len(lens):.2f}\")\n",
    "\n",
    "# --- Show examples ---\n",
    "print(\"\\nSample merged entries:\")\n",
    "for i, (pid, labs) in enumerate(list(merged.items())[:5]):\n",
    "    print(f\"  Product {pid} → labels: {labs}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "# --- Save ---\n",
    "output_path = \"Silver/train_merged_for_selftraining.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved merged dataset to: {output_path}\")\n",
    "\n",
    "# --- Unlabeled count ---\n",
    "unlabeled_count = len(silver_labels) - len(merged)\n",
    "print(f\"\\nUnlabeled products kept aside: {unlabeled_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
