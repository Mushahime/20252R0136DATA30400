{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ebc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json, random, copy\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Load data ---\n",
    "with open(\"Silver/train_merged_for_selftraining.json\", \"r\") as f:\n",
    "    merged_labels = json.load(f)\n",
    "\n",
    "with open(\"Amazon_products/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    id2label = {int(line.split(\"\\t\")[0]): line.split(\"\\t\")[1].strip() for line in f}\n",
    "\n",
    "# suppose you already have your embeddings tensor + pid2idx dict\n",
    "print(\"âš™ï¸ Loading embeddings (precomputed)...\")\n",
    "embeddings = torch.load(\"embeddings.pt\")  # shape [N, D]\n",
    "pid2idx = torch.load(\"pid2idx.pt\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_classes = len(id2label)\n",
    "input_dim = embeddings.size(1)\n",
    "\n",
    "# --- Dataset ---\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, pid2labels, pid2idx, embeddings, num_classes):\n",
    "        self.pids = list(pid2labels.keys())\n",
    "        self.pid2idx = pid2idx\n",
    "        self.embeddings = embeddings\n",
    "        self.num_classes = num_classes\n",
    "        self.labels = [pid2labels[pid] for pid in self.pids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.pids[idx]\n",
    "        emb = self.embeddings[self.pid2idx[pid]]\n",
    "        y = torch.zeros(self.num_classes)\n",
    "        for lid in self.labels[idx]:\n",
    "            y[lid] = 1.0\n",
    "        return {\"X\": emb, \"y\": y, \"pid\": pid}\n",
    "\n",
    "# Split train/val\n",
    "pids = list(merged_labels.keys())\n",
    "train_pids, val_pids = train_test_split(pids, test_size=0.15, random_state=42)\n",
    "train_labels = {pid: merged_labels[pid] for pid in train_pids}\n",
    "val_labels = {pid: merged_labels[pid] for pid in val_pids}\n",
    "print(f\"âœ… Train: {len(train_labels)} | Val: {len(val_labels)}\")\n",
    "\n",
    "train_ds = MultiLabelDataset(train_labels, pid2idx, embeddings, num_classes)\n",
    "val_ds = MultiLabelDataset(val_labels, pid2idx, embeddings, num_classes)\n",
    "train_ld = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# --- Model ---\n",
    "class InnerProductClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, 768)\n",
    "        self.label_emb = nn.Parameter(torch.randn(num_classes, 768))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.proj(x)))\n",
    "        logits = torch.matmul(x, self.label_emb.T)\n",
    "        return logits\n",
    "\n",
    "model = InnerProductClassifier(input_dim, num_classes).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            X, y = batch[\"X\"].to(device), batch[\"y\"].to(device)\n",
    "            logits = model(X)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_true.append(y.cpu())\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_true = torch.cat(all_true)\n",
    "    f1 = f1_score(all_true, all_preds, average=\"micro\", zero_division=0)\n",
    "    return f1\n",
    "\n",
    "# --- Loss ---\n",
    "def multilabel_loss(logits, targets):\n",
    "    return F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\").sum(dim=1).mean()\n",
    "\n",
    "# --- Phase 1: base training ---\n",
    "print(\"\\n=== Phase 1: Base Training ===\")\n",
    "best_f1 = 0\n",
    "patience, wait = 5, 0\n",
    "best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_ld, desc=f\"Epoch {epoch}\"):\n",
    "        X, y = batch[\"X\"].to(device), batch[\"y\"].to(device)\n",
    "        logits = model(X)\n",
    "        loss = multilabel_loss(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    f1_val = evaluate(model, val_ld)\n",
    "    print(f\"Epoch {epoch:2d} | Loss={total_loss/len(train_ld):.3f} | Val F1={f1_val:.4f}\")\n",
    "\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        wait = 0\n",
    "        print(\"  ðŸ”¥ New best model!\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"â¹ï¸ Early stopping.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# --- Save model ---\n",
    "torch.save(model.state_dict(), \"model_selftrain_base.pt\")\n",
    "print(\"\\nðŸ’¾ Base model saved to model_selftrain_base.pt\")\n",
    "\n",
    "# --- Phase 2 (pseudo-labelling + consistency) ---\n",
    "print(\"\\n=== Phase 2: Self-training === (multi-label)\")\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "alpha_ema = 0.999\n",
    "lambda_cons = 1.5\n",
    "\n",
    "def ema_update(ema_model, model, alpha):\n",
    "    for ema_p, p in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_p.data.mul_(alpha).add_(p.data, alpha=(1 - alpha))\n",
    "\n",
    "def consistency_loss(logits_s, logits_t):\n",
    "    p_s = torch.sigmoid(logits_s)\n",
    "    p_t = torch.sigmoid(logits_t)\n",
    "    return F.mse_loss(p_s, p_t)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_ld, desc=f\"ST Epoch {epoch}\"):\n",
    "        X, y = batch[\"X\"].to(device), batch[\"y\"].to(device)\n",
    "        noise = torch.randn_like(X) * 0.05\n",
    "        logits_s = model(X + noise)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(X)\n",
    "        loss = multilabel_loss(logits_s, y) + lambda_cons * consistency_loss(logits_s, logits_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema_update(teacher, model, alpha_ema)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    f1_val = evaluate(teacher, val_ld)\n",
    "    print(f\"ST Epoch {epoch:2d} | Loss={total_loss/len(train_ld):.3f} | Val F1={f1_val:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Self-training phase done! Model stabilized.\")\n",
    "torch.save(teacher.state_dict(), \"model_selftrain_final.pt\")\n",
    "print(\"ðŸ’¾ Saved final teacher model -> model_selftrain_final.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
