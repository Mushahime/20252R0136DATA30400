{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d718635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(\"Amazon_products\")\n",
    "TRAIN_CORPUS_PATH = ROOT / \"train\" / \"train_corpus.txt\"\n",
    "TEST_CORPUS_PATH  = ROOT / \"test\" / \"test_corpus.txt\"\n",
    "CLASS_PATH        = ROOT / \"classes.txt\"\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\" \n",
    "\n",
    "EMB_DIR          = Path(\"Embeddings\")\n",
    "X_ALL_PATH       = EMB_DIR / \"X_train_test_mpn.pt\"\n",
    "LABEL_EMB_PATH   = EMB_DIR / \"labels_hierarchical_new_mpn.pt\"\n",
    "\n",
    "MODEL_SAVE = Path(\"Models\")\n",
    "MODEL_SAVE.mkdir(exist_ok=True)\n",
    "MODEL_PATH = MODEL_SAVE / \"silver_classifier.pt\"\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD IDS\n",
    "# ==========================================================\n",
    "def load_ids(path):\n",
    "    ids = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            pid, _ = line.strip().split(\"\\t\", 1)\n",
    "            ids.append(int(pid))\n",
    "    return ids\n",
    "\n",
    "train_ids = load_ids(TRAIN_CORPUS_PATH)\n",
    "test_ids  = load_ids(TEST_CORPUS_PATH)\n",
    "n_train = len(train_ids)\n",
    "n_test  = len(test_ids)\n",
    "\n",
    "print(f\"Train IDs: {n_train} | Test IDs: {n_test}\")\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"Load multi-label data into {id: [labels]} dictionary.\"\"\"\n",
    "    id2labels = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, label = parts\n",
    "                pid = int(pid)\n",
    "                label = int(label)\n",
    "\n",
    "                if pid not in id2labels:\n",
    "                    id2labels[pid] = []\n",
    "\n",
    "                id2labels[pid].append(label)\n",
    "    return id2labels\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD X_all\n",
    "# ==========================================================\n",
    "print(\"\\nüß† Loading X_all.pt ...\")\n",
    "data = torch.load(X_ALL_PATH, weights_only=False)\n",
    "\n",
    "if isinstance(data, np.ndarray):\n",
    "    data = torch.from_numpy(data)\n",
    "elif isinstance(data, list):\n",
    "    data = torch.stack(data)\n",
    "\n",
    "X_all = data.float().to(device)\n",
    "assert X_all.shape[0] == n_train + n_test\n",
    "\n",
    "X_train = X_all[:n_train]\n",
    "X_test  = X_all[n_train:]\n",
    "print(f\"‚úì X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD LABEL EMBEDDINGS\n",
    "# ==========================================================\n",
    "tmp = torch.load(LABEL_EMB_PATH, weights_only=False)\n",
    "\n",
    "# Convertir numpy ‚Üí tensor si n√©cessaire\n",
    "if isinstance(tmp, np.ndarray):\n",
    "    tmp = torch.from_numpy(tmp)\n",
    "\n",
    "label_emb = tmp.float().to(device)\n",
    "print(f\"‚úì Label embeddings: {label_emb.shape}\")\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD CLASS NAMES\n",
    "# ==========================================================\n",
    "classes = {}\n",
    "with open(CLASS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        cid, cname = line.strip().split(\"\\t\")\n",
    "        classes[int(cid)] = cname\n",
    "\n",
    "n_classes = len(classes)\n",
    "\n",
    "pid2idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "print(class2hierarchy)\n",
    "\n",
    "import json\n",
    "\n",
    "# Load JSON\n",
    "with open(\"Silver/silver_train_new_mpn.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "confidence_threshold = 0.62\n",
    "\n",
    "pid2labelids_silver_filtered = {}\n",
    "\n",
    "for pid_str, data in raw.items():\n",
    "    pid = int(pid_str)\n",
    "\n",
    "    labels = data[\"labels\"]\n",
    "    probs = data[\"probs\"]\n",
    "\n",
    "    # Si AU MOINS un score d√©passe le seuil ‚Üí on garde TOUTE la liste\n",
    "    if any(score > confidence_threshold for score in probs):\n",
    "        pid2labelids_silver_filtered[pid] = labels\n",
    "\n",
    "print(f\"Filtered: {len(pid2labelids_silver_filtered)} / {len(raw)}\")\n",
    "\n",
    "silver_labels = pid2labelids_silver_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04658037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, pids, labels_dict):\n",
    "        self.pids = pids\n",
    "        self.labels = labels_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.pids[idx]\n",
    "        emb = X_train[pid2idx[pid]]\n",
    "\n",
    "        y = torch.zeros(n_classes)\n",
    "        for c in self.labels[pid]:\n",
    "            if 0 <= c < n_classes:\n",
    "                y[c] = 1.0\n",
    "\n",
    "        return {\"X\": emb, \"y\": y}\n",
    "    \n",
    "\n",
    "class UnlabeledEmbeddingDataset(Dataset):\n",
    "    def __init__(self, pids, pid2idx, embeddings):\n",
    "        self.pids = pids                \n",
    "        self.pid2idx = pid2idx             \n",
    "        self.embeddings = embeddings      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.pids[idx]\n",
    "        emb = self.embeddings[self.pid2idx[pid]]\n",
    "\n",
    "        return {\"X\": emb, \"pid\": pid}\n",
    "\n",
    "\n",
    "train_p, val_p = train_test_split(\n",
    "    list(silver_labels.keys()), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = MultiLabelDataset(train_p, silver_labels)\n",
    "val_dataset   = MultiLabelDataset(val_p, silver_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64)\n",
    "\n",
    "class LabelGCN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W_list = nn.ParameterList()\n",
    "        for _ in range(num_layers):\n",
    "            W = nn.Parameter(torch.empty(emb_dim, emb_dim))\n",
    "            nn.init.xavier_uniform_(W)\n",
    "            self.W_list.append(W)\n",
    "\n",
    "    def forward(self, H, A_hat):\n",
    "        for i, W in enumerate(self.W_list):\n",
    "            H_input = H  # skip connection\n",
    "\n",
    "            H_msg = A_hat @ H_input\n",
    "            H_msg = H_msg @ W\n",
    "\n",
    "            # residual connection\n",
    "            H = H_input + H_msg\n",
    "\n",
    "            if i < self.num_layers - 1:\n",
    "                H = F.relu(H)\n",
    "                H = F.dropout(H, p=self.dropout, training=self.training)\n",
    "\n",
    "        return H\n",
    "\n",
    "\n",
    "class GCNEnhancedClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, label_init_emb, A_hat, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        emb_dim = label_init_emb.size(1)\n",
    "\n",
    "        # proj docs -> label space\n",
    "        self.proj = nn.Linear(input_dim, emb_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # GNN sur les labels\n",
    "        self.encoder = LabelGCN(emb_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "        # label embeddings trainables\n",
    "        self.label_emb = nn.Parameter(label_init_emb.clone())\n",
    "\n",
    "        # matrice d‚Äôadjacence (buffer, pas un param√®tre)\n",
    "        self.register_buffer(\"A_hat\", A_hat)\n",
    "\n",
    "    def forward(self, x, use_dropout=True):\n",
    "        # 1) raffiner les embeddings de labels\n",
    "        E_refine = self.encoder(self.label_emb, self.A_hat)   # (C, D)\n",
    "\n",
    "        # 2) projeter les docs\n",
    "        x_proj = self.proj(x)\n",
    "        if use_dropout:\n",
    "            x_proj = F.dropout(x_proj, p=self.dropout, training=self.training)\n",
    "\n",
    "        # 3) logits = produit scalaire\n",
    "        logits = x_proj @ E_refine.T    # (B, C)\n",
    "        return logits\n",
    "    \n",
    "def evaluate(model, loader, thr=0.25):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            X = batch[\"X\"]\n",
    "            y = batch[\"y\"].numpy()\n",
    "\n",
    "            prob = torch.sigmoid(model(X)).cpu().numpy()\n",
    "            pred = (prob > thr).astype(int)\n",
    "\n",
    "            preds.extend(pred)\n",
    "            labels.extend(y)\n",
    "\n",
    "    f1s = f1_score(labels, preds, average=\"samples\")\n",
    "    f1m = f1_score(labels, preds, average=\"macro\")\n",
    "    f1mic = f1_score(labels, preds, average=\"micro\")\n",
    "    return f1s, f1m, f1mic\n",
    "\n",
    "\n",
    "def build_adj_from_hierarchy(class2hierarchy, n_classes, w_parent=1.0, w_sibling=0.1):\n",
    "    \"\"\"\n",
    "    Construit A_hat pour GCN en utilisant EXCLUSIVEMENT class2hierarchy.\n",
    "\n",
    "    - parent <-> enfant : poids = w_parent\n",
    "    - fr√®res/soeurs : poids = w_sibling\n",
    "    - auto-boucle : 1.0 (standard GCN)\n",
    "    \"\"\"\n",
    "\n",
    "    A = torch.zeros((n_classes, n_classes))\n",
    "\n",
    "    # ---- liens parent/enfant + siblings ----\n",
    "    for parent, children in class2hierarchy.items():\n",
    "\n",
    "        # parent <-> enfant\n",
    "        for c in children:\n",
    "            A[parent, c] = w_parent\n",
    "            A[c, parent] = w_parent\n",
    "\n",
    "        # siblings (enfants du m√™me parent)\n",
    "        for i in range(len(children)):\n",
    "            for j in range(i + 1, len(children)):\n",
    "                c1, c2 = children[i], children[j]\n",
    "                A[c1, c2] = w_sibling\n",
    "                A[c2, c1] = w_sibling\n",
    "\n",
    "    # ---- self-loops ----\n",
    "    A = A + torch.eye(n_classes)\n",
    "\n",
    "    # ---- normalisation GCN ----\n",
    "    D = A.sum(dim=1)\n",
    "    D_inv_sqrt = torch.pow(D, -0.5)\n",
    "    D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "    D_mat = torch.diag(D_inv_sqrt)\n",
    "\n",
    "    A_hat = D_mat @ A @ D_mat\n",
    "    return A_hat\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"\n",
    "    Charge un fichier parent-enfant du type :\n",
    "    parent_id \\t child_id\n",
    "\n",
    "    Retourne :\n",
    "    {parent: [child, ...]}\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            p, c = line.strip().split(\"\\t\")\n",
    "            p, c = int(p), int(c)\n",
    "\n",
    "            if p not in mapping:\n",
    "                mapping[p] = []\n",
    "\n",
    "            mapping[p].append(c)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# ---------- CHARGEMENT HI√âRARCHIE ----------\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\"\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "\n",
    "A_hat = build_adj_from_hierarchy(class2hierarchy, n_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "best = 0\n",
    "epochs = 8\n",
    "wait = 0\n",
    "patience = 8\n",
    "\n",
    "model = GCNEnhancedClassifier(\n",
    "    input_dim=X_train.size(1),\n",
    "    label_init_emb=label_emb,\n",
    "    A_hat=A_hat,\n",
    "    num_layers=3,     \n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "\n",
    "    f1s, f1m, f1mic = evaluate(model, val_loader)\n",
    "    print(f\"[Epoch {epoch}] loss={total/len(train_loader):.4f} | F1={f1s:.4f}\")\n",
    "\n",
    "    if f1s > best:\n",
    "        best = f1s\n",
    "        wait = 0\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"New best model saved ({best:.4f})\")\n",
    "    else:\n",
    "        wait += 1\n",
    "    \n",
    "    if wait >= patience:\n",
    "        print(\"\\nEarly stopping triggered!\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"\\nBest validation F1 = {best:.4f}\")\n",
    "print(f\"Model saved at: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load best student before test ===\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "\n",
    "student = model\n",
    "\n",
    "teacher = GCNEnhancedClassifier(\n",
    "    input_dim=X_train.size(1),\n",
    "    label_init_emb=label_emb,\n",
    "    A_hat=A_hat,\n",
    "    num_layers=3,     \n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "teacher.load_state_dict(best_model) \n",
    "teacher.eval()\n",
    "\n",
    "def ema_update(teacher, student, alpha=0.999):\n",
    "    for t_param, s_param in zip(teacher.parameters(), student.parameters()):\n",
    "        t_param.data.mul_(alpha).add_(s_param.data * (1 - alpha))\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "pseudo_update_freq = 5\n",
    "\n",
    "threshold_start = 0.65\n",
    "threshold_end   = 0.8\n",
    "\n",
    "alpha_ema = 0.99\n",
    "lambda_cons = 1.5\n",
    "\n",
    "current_labels = dict(silver_labels) \n",
    "all_train_pids = list(pid2idx.keys())\n",
    "\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "wait = 0\n",
    "patience = 5\n",
    "best_f1 = 0.\n",
    "best_teacher = copy.deepcopy(teacher.state_dict())\n",
    "\n",
    "def consistency_loss(log_s, log_t):\n",
    "    ps = torch.sigmoid(log_s)\n",
    "    pt = torch.sigmoid(log_t)\n",
    "    return F.mse_loss(ps, pt)\n",
    "\n",
    "\n",
    "def expand_with_hierarchy(labels, hierarchy):\n",
    "    \"\"\"\n",
    "    Expand a list of core labels by adding ALL their ancestors\n",
    "    (parents, parents of parents, etc.), recursively.\n",
    "    This guarantees 100% hierarchy consistency.\n",
    "    \"\"\"\n",
    "    expanded = set(labels)\n",
    "    stack = list(labels)\n",
    "    child2parents = {}\n",
    "    for parent, children in hierarchy.items():\n",
    "        for child in children:\n",
    "            child2parents.setdefault(child, []).append(parent)\n",
    "\n",
    "    # DFS / BFS upward through ancestors\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "\n",
    "        if node not in child2parents:\n",
    "            continue\n",
    "\n",
    "        for parent in child2parents[node]:\n",
    "            if parent not in expanded:\n",
    "                expanded.add(parent)\n",
    "                stack.append(parent)  \n",
    "\n",
    "    return sorted(expanded)[-3:]\n",
    "\n",
    "\n",
    "def select_labels_hierarchical(probs, threshold, hierarchy, max_k=3):\n",
    "    # 1) labels surpassant le seuil\n",
    "    cand = [i for i in range(len(probs)) if probs[i] > threshold]\n",
    "\n",
    "    if len(cand) == 0:\n",
    "        return []\n",
    "\n",
    "    # ordonner par probas\n",
    "    cand = sorted(cand, key=lambda c: -probs[c])\n",
    "    # prendre au maximum 3 feuilles candidates\n",
    "    cand = cand[:max_k]\n",
    "\n",
    "    # 2) expansion hi√©rarchique\n",
    "    expanded = set(cand)\n",
    "    for c in cand:\n",
    "        if str(c) in hierarchy:\n",
    "            parents = hierarchy[str(c)]\n",
    "            for p in parents:\n",
    "                expanded.add(p)\n",
    "\n",
    "    expanded = list(expanded)\n",
    "\n",
    "    # 3) garder max 3 labels au total\n",
    "    expanded = sorted(expanded, key=lambda c: -probs[c])[:max_k]\n",
    "\n",
    "    return expanded\n",
    "\n",
    "\n",
    "def invert_hierarchy(class2hierarchy):\n",
    "    inv = {}\n",
    "    for parent, children in class2hierarchy.items():\n",
    "        for child in children:\n",
    "            inv.setdefault(child, []).append(parent)\n",
    "    return inv\n",
    "\n",
    "inv_hierarchy = invert_hierarchy(class2hierarchy)\n",
    "\n",
    "\n",
    "def generate_pseudo_labels(threshold):\n",
    "    teacher.eval()\n",
    "    new_pseudo = {}\n",
    "\n",
    "    labeled = set(current_labels.keys())\n",
    "    unlabeled_pids = [pid for pid in all_train_pids if pid not in labeled]\n",
    "\n",
    "    unlabeled_ds = UnlabeledEmbeddingDataset(unlabeled_pids, pid2idx, X_train)\n",
    "    unlabeled_ld = DataLoader(unlabeled_ds, batch_size=64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in unlabeled_ld:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            pids = batch[\"pid\"]\n",
    "\n",
    "            logits = teacher(X)\n",
    "            probs_batch = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            for pid, p in zip(pids, probs_batch):\n",
    "\n",
    "                final_labels = select_labels_hierarchical(\n",
    "                    probs=p,\n",
    "                    threshold=threshold,\n",
    "                    hierarchy=inv_hierarchy,\n",
    "                    max_k=3\n",
    "                )\n",
    "\n",
    "                if len(final_labels) < 2:\n",
    "                    continue\n",
    "\n",
    "                new_pseudo[int(pid)] = final_labels\n",
    "\n",
    "    return new_pseudo\n",
    "\n",
    "\n",
    "total = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    if epoch % pseudo_update_freq == 1 and epoch > 1:\n",
    "        progress = epoch / EPOCHS\n",
    "        thr = threshold_start + (threshold_end - threshold_start) * (epoch / EPOCHS)**2\n",
    "\n",
    "        new_pseudo = generate_pseudo_labels(thr)\n",
    "\n",
    "        if len(new_pseudo) > 0:\n",
    "            before = len(current_labels)\n",
    "            current_labels.update(new_pseudo)\n",
    "            print(f\"Added {len(current_labels) - before} pseudo-labeled examples\")\n",
    "            total += len(current_labels) - before\n",
    "\n",
    "    train_ds = MultiLabelDataset(list(current_labels.keys()), current_labels)\n",
    "    train_ld = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    for batch in train_ld:\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        # student (noisy)\n",
    "        noise = torch.randn_like(X) * 0.05\n",
    "        logits_s = student(X + noise)\n",
    "\n",
    "        # teacher (clean)\n",
    "        with torch.no_grad():\n",
    "            logits_t = teacher(X)\n",
    "\n",
    "        # Multi-label supervised loss\n",
    "        loss_sup = F.binary_cross_entropy_with_logits(logits_s, y)\n",
    "\n",
    "        # Consistency loss\n",
    "        loss_cons = consistency_loss(logits_s, logits_t)\n",
    "        loss = loss_sup + lambda_cons * loss_cons\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA teacher update\n",
    "        ema_update(teacher, student, alpha_ema)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation with teacher\n",
    "    teacher.eval()\n",
    "    f1s, f1m, f1mic = evaluate(teacher, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss={epoch_loss/len(train_ld):.4f} | F1={f1s:.4f}\")\n",
    "\n",
    "    if f1s > best_f1:\n",
    "        best_f1 = f1s\n",
    "        best_teacher = copy.deepcopy(teacher.state_dict())\n",
    "        wait = 0\n",
    "        print(f\"New best F1 = {best_f1:.4f}\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            print(f\"Number of PS added {total}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1de81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best teacher\n",
    "teacher.load_state_dict(best_teacher)\n",
    "print(\"\\nFinal teacher F1:\", best_f1)\n",
    "\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nüìù Generating submission...\")\n",
    "\n",
    "teacher.eval()\n",
    "X_test = X_test.to(device)\n",
    "\n",
    "def select_k(prob, min_k=2, max_k=3):\n",
    "    idx = np.argsort(prob)[::-1]    # sorted descending\n",
    "\n",
    "    # Always take the best 3 candidates\n",
    "    top3 = idx[:max_k]\n",
    "\n",
    "    # If the 3rd is much weaker ‚Üí keep only 2\n",
    "    if prob[top3[2]] < 0.25 * prob[top3[1]]:\n",
    "        return top3[:2]\n",
    "\n",
    "    return top3\n",
    "\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(X_test), 64)):\n",
    "        batch = X_test[start:start+64]\n",
    "\n",
    "        logits = teacher(batch, use_dropout=False)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        for p in probs:\n",
    "            labels = select_k(p)\n",
    "            preds.append([str(x) for x in labels])\n",
    "\n",
    "# ==========================================================\n",
    "# SAVE CSV\n",
    "# ==========================================================\n",
    "\n",
    "OUT_DIR = Path(\"Submission\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "OUT_PATH = OUT_DIR / \"submission_selfGNN.csv\"\n",
    "\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"id\",\"label\"])\n",
    "    for pid, labels in zip(test_ids, preds):\n",
    "        w.writerow([pid, \",\".join(labels)])\n",
    "\n",
    "print(f\"üéâ Submission saved ‚Üí {OUT_PATH}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
