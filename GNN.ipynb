{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d02b2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train: 29487 samples\n",
      "Test: 19658 samples\n",
      "Classes: 531\n",
      "Train embeddings: torch.Size([29487, 768])\n",
      "Test embeddings: torch.Size([19658, 768])\n",
      "Train labels: torch.Size([29487, 531])\n",
      "Label embeddings: torch.Size([531, 768])\n",
      "Input dimension: 768\n",
      "Num classes: 531\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "# Seed pour reproductibilit√©\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "ROOT = Path(\"Amazon_products\")\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "\n",
    "TEST_CORPUS_PATH = TEST_DIR / \"test_corpus.txt\"\n",
    "TRAIN_CORPUS_PATH = TRAIN_DIR / \"train_corpus.txt\"\n",
    "\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\"\n",
    "CLASS_RELATED_PATH = ROOT / \"class_related_keywords.txt\"\n",
    "CLASS_PATH = ROOT / \"classes.txt\"\n",
    "\n",
    "SUBMISSION_PATH = \"Submission/submission.csv\"\n",
    "\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load corpus into {id: text} dictionary.\"\"\"\n",
    "    id2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                id, text = parts\n",
    "                id2text[id] = text\n",
    "    return id2text\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"Load multi-label data into {id: [labels]} dictionary.\"\"\"\n",
    "    id2labels = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, label = parts\n",
    "                pid = int(pid)\n",
    "                label = int(label)\n",
    "                if pid not in id2labels:\n",
    "                    id2labels[pid] = []\n",
    "                id2labels[pid].append(label)\n",
    "    return id2labels\n",
    "\n",
    "def load_class_keywords(path):\n",
    "    \"\"\"Load class keywords into {class_name: [keywords]} dictionary.\"\"\"\n",
    "    class2keywords = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            classname, keywords = line.strip().split(\":\", 1)\n",
    "            keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
    "            class2keywords[classname] = keyword_list\n",
    "    return class2keywords\n",
    "\n",
    "id2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "id2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "\n",
    "# Classes\n",
    "id2class = load_corpus(CLASS_PATH)\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "class2related = load_class_keywords(CLASS_RELATED_PATH)\n",
    "\n",
    "# Silver labels (RoBERTa - les meilleurs)\n",
    "with open(\"Silver/silver_train_roberta.json\", \"r\") as f:\n",
    "    pid2labelids_silver = json.load(f)\n",
    "\n",
    "with open(\"Silver/silver_test_roberta.json\", \"r\") as f:\n",
    "    pid2labelids_test = json.load(f)\n",
    "\n",
    "print(f\"Train: {len(id2text_train)} samples\")\n",
    "print(f\"Test: {len(id2text_test)} samples\")\n",
    "print(f\"Classes: {len(id2class)}\")\n",
    "\n",
    "X_train = torch.load(\"Embeddings/X_train.pt\").to(device)\n",
    "y_train = torch.load(\"Embeddings/y_train.pt\").to(device)\n",
    "X_test = torch.load(\"Embeddings/X_test.pt\").to(device)\n",
    "label_emb = torch.load(\"Embeddings/label_emb.pt\").to(device)\n",
    "test_ids = list(id2text_test.keys())\n",
    "train_ids = list(id2text_train.keys())\n",
    "\n",
    "print(f\"Train embeddings: {X_train.shape}\")\n",
    "print(f\"Test embeddings: {X_test.shape}\")\n",
    "print(f\"Train labels: {y_train.shape}\")\n",
    "print(f\"Label embeddings: {label_emb.shape}\")\n",
    "\n",
    "# Index mapping\n",
    "pid2idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "input_dim = X_train.size(1)\n",
    "num_classes = y_train.size(1)\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Num classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458e2127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: torch.Size([531, 531])\n",
      "D shape: torch.Size([531, 531])\n",
      "A_hat shape: torch.Size([531, 531])\n"
     ]
    }
   ],
   "source": [
    "# Build A and D\n",
    "\n",
    "# Use the number of classes inferred from y_train for consistency\n",
    "num_classes = y_train.size(1)\n",
    "\n",
    "# Adjacency matrix (num_classes x num_classes)\n",
    "A = torch.zeros((num_classes, num_classes), dtype=torch.float32, device=device)\n",
    "\n",
    "# class2hierarchy is a dict {class_id: [related_class_ids]}\n",
    "# We'll connect nodes symmetrically (u <-> v)\n",
    "for cls, related_list in class2hierarchy.items():\n",
    "    if cls >= num_classes:\n",
    "        continue\n",
    "    for other in related_list:\n",
    "        if other >= num_classes:\n",
    "            continue\n",
    "        A[cls, other] = 1.0\n",
    "        A[other, cls] = 1.0  # make the graph undirected\n",
    "\n",
    "# Add self-loops (each class connected to itself)\n",
    "A = A + torch.eye(num_classes, device=device)\n",
    "\n",
    "# Degree matrix D (diagonal matrix of node degrees)\n",
    "deg = A.sum(dim=1)  # (num_classes,)\n",
    "D = torch.diag(deg)\n",
    "\n",
    "print(\"A shape:\", A.shape)\n",
    "print(\"D shape:\", D.shape)\n",
    "\n",
    "# Normalized adjacency matrix for GCN: A_hat = D^{-1/2} A D^{-1/2}\n",
    "deg_inv_sqrt = torch.pow(deg, -0.5)\n",
    "deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n",
    "D_inv_sqrt = torch.diag(deg_inv_sqrt)\n",
    "\n",
    "A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "print(\"A_hat shape:\", A_hat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Your Task: Implement Label GCN and GCN-Enhanced Classifier\n",
    "# ==========================================================\n",
    "\n",
    "class LabelGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Graph Convolutional Network (GCN) encoder for label embeddings.\n",
    "\n",
    "    Each layer should perform the following steps:\n",
    "        1. Aggregate neighbor embeddings: H <- A_hat @ H\n",
    "        2. Linear transformation: H <- H @ W\n",
    "        3. (Optional) Apply ReLU and Dropout (skip for the last layer)\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int): Dimension of label embeddings.\n",
    "        num_layers (int): Number of GCN layers.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        # TODO: Define learnable weight matrices (list of emb_dim x emb_dim parameters)\n",
    "        # Hint: Use nn.ParameterList and Xavier uniform initialization\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout = dropout\n",
    "        self.W_list = nn.ParameterList()\n",
    "\n",
    "        for i in range (self.num_layers):\n",
    "            W = nn.Parameter(torch.empty(self.emb_dim, self.emb_dim))\n",
    "            nn.init.xavier_uniform_(W) \n",
    "            self.W_list.append(W)\n",
    "\n",
    "    def forward(self, H, A_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            H (torch.Tensor): Initial label embeddings, shape (num_labels, emb_dim).\n",
    "            A_hat (torch.Tensor): Normalized adjacency matrix, shape (num_labels, num_labels).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated label embeddings, shape (num_labels, emb_dim).\n",
    "        \"\"\"\n",
    "        # TODO: Implement multi-layer GCN\n",
    "        # for each layer:\n",
    "        #   1) propagate messages: H = A_hat @ H\n",
    "        #   2) linear transform: H = H @ W\n",
    "        #   3) if not last layer: apply ReLU + Dropout\n",
    "\n",
    "        for i in range (len(self.W_list)):\n",
    "            H = torch.matmul(torch.matmul(A_hat, H), self.W_list[i]) # H = A_hat @ H @ self.W_list[i]\n",
    "            if i < self.num_layers -1:\n",
    "                H = F.relu(H)\n",
    "                H = F.dropout(H, p=self.dropout, training=self.training)\n",
    "\n",
    "        return H\n",
    "\n",
    "\n",
    "class GCNEnhancedClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier that combines:\n",
    "      - Document representations projected into label space\n",
    "      - Label embeddings refined by a GCN over the label graph\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of input document embeddings.\n",
    "        label_init_emb (torch.Tensor): Initial label embeddings, shape (num_labels, emb_dim).\n",
    "        A_hat (torch.Tensor): Normalized adjacency matrix of labels, shape (num_labels, num_labels).\n",
    "        num_layers (int): Number of GCN layers.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, label_init_emb, A_hat, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, label_init_emb.size(1)) #1\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = LabelGCN(label_init_emb.size(1), num_layers, dropout) #2\n",
    "\n",
    "        self.label_emb = nn.Parameter(label_init_emb.clone()) #3\n",
    "\n",
    "        self.register_buffer(\"A_hat\", A_hat) #4\n",
    "\n",
    "        # TODO: \n",
    "        # 1. Define projection layer (input_dim -> emb_dim)\n",
    "        # 2. Define GCN encoder for label embeddings\n",
    "        # 3. Make label_init_emb trainable (nn.Parameter)\n",
    "        # 4. Register adjacency matrix (use register_buffer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings for documents, shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for classification, shape (batch_size, num_labels).\n",
    "        \"\"\"\n",
    "        E_refine = self.encoder(self.label_emb, self.A_hat) #1\n",
    "\n",
    "        x_proj = self.proj(x)\n",
    "        x_proj = F.dropout(x_proj, p=self.dropout, training=self.training) #2\n",
    "\n",
    "        logits = torch.matmul(x_proj, E_refine.T) #3\n",
    "        return logits\n",
    "\n",
    "        # TODO:\n",
    "        # 1. Refine label embeddings using GCN\n",
    "        # 2. Project input x into label embedding space (+ dropout)\n",
    "        # 3. Compute similarity (inner product) between x_proj and label_emb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = GCNEnhancedClassifier(embeddings.size(1), label_emb, A_hat.to(device), num_layers=2).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "best_val_acc = -1\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # === Validation ===\n",
    "    val_result = evaluate(model, val_loader, device=device)\n",
    "    val_acc = val_result[\"accuracy\"]\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    is_improved = val_acc > best_val_acc\n",
    "    print_eval_result(val_result, stage=\"val\", is_improved=is_improved)\n",
    "\n",
    "    # === Test ===\n",
    "    test_result = evaluate(model, test_loader, device=device)\n",
    "    test_acc = test_result[\"accuracy\"]\n",
    "    test_acc_list.append(test_acc)\n",
    "    print_eval_result(test_result, stage=\"test\")\n",
    "\n",
    "    # === Update best model ===\n",
    "    if is_improved:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # === Early stopping ===\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"[Early Stopping] No improvement for {patience} consecutive epochs.\")\n",
    "        break\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
