{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d02b2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train: 29487 samples\n",
      "Classes: 531\n",
      "Train embeddings: torch.Size([29487, 768])\n",
      "Test embeddings: torch.Size([19658, 768])\n",
      "Label embeddings: torch.Size([531, 768])\n",
      "Input dimension: 768\n",
      "Num classes: 531\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "ROOT = Path(\"Amazon_products\")\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "\n",
    "TEST_CORPUS_PATH = TEST_DIR / \"test_corpus.txt\"\n",
    "TRAIN_CORPUS_PATH = TRAIN_DIR / \"train_corpus.txt\"\n",
    "\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\"\n",
    "CLASS_RELATED_PATH = ROOT / \"class_related_keywords.txt\"\n",
    "CLASS_PATH = ROOT / \"classes.txt\"\n",
    "\n",
    "SUBMISSION_PATH = \"Submission/submission.csv\"\n",
    "\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n",
    "\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load corpus into {id: text} dictionary.\"\"\"\n",
    "    id2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                id, text = parts\n",
    "                id2text[id] = text\n",
    "    return id2text\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"Load multi-label data into {id: [labels]} dictionary.\"\"\"\n",
    "    id2labels = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, label = parts\n",
    "                pid = int(pid)\n",
    "                label = int(label)\n",
    "                if pid not in id2labels:\n",
    "                    id2labels[pid] = []\n",
    "                id2labels[pid].append(label)\n",
    "    return id2labels\n",
    "\n",
    "def load_class_keywords(path):\n",
    "    \"\"\"Load class keywords into {class_name: [keywords]} dictionary.\"\"\"\n",
    "    class2keywords = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            classname, keywords = line.strip().split(\":\", 1)\n",
    "            keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
    "            class2keywords[classname] = keyword_list\n",
    "    return class2keywords\n",
    "\n",
    "id2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "id2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "\n",
    "# Classes\n",
    "id2class = load_corpus(CLASS_PATH)\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "class2related = load_class_keywords(CLASS_RELATED_PATH)\n",
    "\n",
    "\n",
    "print(f\"Train: {len(id2text_train)} samples\")\n",
    "print(f\"Classes: {len(id2class)}\")\n",
    "\n",
    "# Embeddings\n",
    "X_train = torch.load(\"Embeddings/X_train.pt\").to(device)\n",
    "X_test = torch.load(\"Embeddings/X_test.pt\").to(device)\n",
    "label_emb = torch.load(\"Embeddings/label_emb.pt\").to(device)\n",
    "test_ids = list(id2text_test.keys())\n",
    "train_ids = list(id2text_train.keys())\n",
    "\n",
    "print(f\"Train embeddings: {X_train.shape}\")\n",
    "print(f\"Test embeddings: {X_test.shape}\")\n",
    "print(f\"Label embeddings: {label_emb.shape}\")\n",
    "\n",
    "# Index mapping\n",
    "pid2idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "input_dim = X_train.size(1)\n",
    "num_classes = NUM_CLASSES\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Num classes: {num_classes}\")\n",
    "\n",
    "\n",
    "with open(\"Silver/silver_fixed.json\", \"r\") as f:\n",
    "    raw_silver = json.load(f)\n",
    "\n",
    "if \"silver_labels\" in raw_silver:\n",
    "    silver_data = raw_silver[\"silver_labels\"]\n",
    "else:\n",
    "    silver_data = raw_silver  # fallback si le fichier ne contient que les labels\n",
    "\n",
    "pid2labelids_silver = {\n",
    "    int(pid): info[\"labels\"]\n",
    "    for pid, info in silver_data.items()\n",
    "    if \"labels\" in info\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458e2127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix A: torch.Size([531, 531])\n",
      "Degree matrix D: torch.Size([531, 531])\n",
      "Normalized adjacency A_hat: torch.Size([531, 531])\n"
     ]
    }
   ],
   "source": [
    "# Adjency matrix A\n",
    "A = torch.zeros((num_classes, num_classes), dtype=torch.float32, device=device)\n",
    "\n",
    "for parent_id, children_ids in class2hierarchy.items():\n",
    "    if parent_id >= num_classes:\n",
    "        continue\n",
    "    for child_id in children_ids:\n",
    "        if child_id >= num_classes:\n",
    "            continue\n",
    "        # Bidirectional link\n",
    "        A[parent_id, child_id] = 1.0\n",
    "        A[child_id, parent_id] = 1.0\n",
    "\n",
    "A = A + torch.eye(num_classes, device=device)\n",
    "\n",
    "# D\n",
    "deg = A.sum(dim=1) \n",
    "D = torch.diag(deg)\n",
    "\n",
    "print(f\"Adjacency matrix A: {A.shape}\")\n",
    "print(f\"Degree matrix D: {D.shape}\")\n",
    "\n",
    "# Normalisation : A_hat = D^{-1/2} @ A @ D^{-1/2}\n",
    "deg_inv_sqrt = torch.pow(deg, -0.5)\n",
    "deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n",
    "D_inv_sqrt = torch.diag(deg_inv_sqrt)\n",
    "\n",
    "A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "print(f\"Normalized adjacency A_hat: {A_hat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e44226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Graph Convolutional Network (GCN) encoder for label embeddings.\n",
    "\n",
    "    Each layer should perform the following steps:\n",
    "        1. Aggregate neighbor embeddings: H <- A_hat @ H\n",
    "        2. Linear transformation: H <- H @ W\n",
    "        3. (Optional) Apply ReLU and Dropout (skip for the last layer)\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int): Dimension of label embeddings.\n",
    "        num_layers (int): Number of GCN layers.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.W_list = nn.ParameterList()\n",
    "        for _ in range(num_layers):\n",
    "            W = nn.Parameter(torch.empty(emb_dim, emb_dim))\n",
    "            nn.init.xavier_uniform_(W)\n",
    "            self.W_list.append(W)\n",
    "\n",
    "    def forward(self, H, A_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            H (torch.Tensor): Initial label embeddings, shape (num_labels, emb_dim).\n",
    "            A_hat (torch.Tensor): Normalized adjacency matrix, shape (num_labels, num_labels).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated label embeddings, shape (num_labels, emb_dim).\n",
    "        \"\"\"\n",
    "        for i, W in enumerate(self.W_list):\n",
    "            H = A_hat @ H @ W\n",
    "            \n",
    "            if i < self.num_layers - 1:\n",
    "                H = F.relu(H)\n",
    "                H = F.dropout(H, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return H\n",
    "\n",
    "\n",
    "class GCNEnhancedClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier that combines:\n",
    "      - Document representations projected into label space\n",
    "      - Label embeddings refined by a GCN over the label graph\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of input document embeddings.\n",
    "        label_init_emb (torch.Tensor): Initial label embeddings, shape (num_labels, emb_dim).\n",
    "        A_hat (torch.Tensor): Normalized adjacency matrix of labels, shape (num_labels, num_labels).\n",
    "        num_layers (int): Number of GCN layers.\n",
    "        dropout (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, label_init_emb, A_hat, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, label_init_emb.size(1)) #1\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = LabelGCN(label_init_emb.size(1), num_layers, dropout) #2\n",
    "\n",
    "        self.label_emb = nn.Parameter(label_init_emb.clone()) #3\n",
    "\n",
    "        self.register_buffer(\"A_hat\", A_hat) #4\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings for documents, shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for classification, shape (batch_size, num_labels).\n",
    "        \"\"\"\n",
    "        E_refine = self.encoder(self.label_emb, self.A_hat) #1\n",
    "\n",
    "        x_proj = self.proj(x)\n",
    "        x_proj = F.dropout(x_proj, p=self.dropout, training=self.training) #2\n",
    "\n",
    "        logits = torch.matmul(x_proj, E_refine.T) #3\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabd3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductCategoryDataset(Dataset):\n",
    "    \"\"\"Dataset using pre-calculated embeddings (train or test compatible)\"\"\"\n",
    "    def __init__(self, pid2label, pid2idx, embeddings, num_classes=531):\n",
    "        self.pid2label = pid2label\n",
    "        self.pid2idx = pid2idx\n",
    "        self.embeddings = embeddings\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if self.pid2label is not None:\n",
    "            self.pids = list(pid2label.keys())\n",
    "            self.has_labels = True\n",
    "        else:\n",
    "            self.pids = list(pid2idx.keys())\n",
    "            self.has_labels = False\n",
    "\n",
    "        self.indices = [pid2idx[pid] for pid in self.pids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[self.indices[idx]]\n",
    "\n",
    "        if self.has_labels:\n",
    "            y = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "            for label in self.pid2label[self.pids[idx]]:\n",
    "                y[label] = 1.0\n",
    "            return {\"X\": emb, \"y\": y}\n",
    "        else:\n",
    "            return {\"X\": emb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da90aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 23589 | Val: 5898\n",
      "âœ“ Train valid: 23589 | Val valid: 5898\n",
      "âš ï¸ Missing 0 PIDs (ex: [])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Load mappings ===\n",
    "pid2idx = torch.load(\"Embeddings/pid2idx.pt\")  # âœ… recharge le bon mapping\n",
    "\n",
    "# === Load Silver Labels ===\n",
    "with open(\"Silver/silver_fixed.json\", \"r\") as f:\n",
    "    raw_silver = json.load(f)\n",
    "\n",
    "pid2labelids_silver = {\n",
    "    int(pid): info[\"labels\"]\n",
    "    for pid, info in raw_silver[\"silver_labels\"].items()\n",
    "}\n",
    "\n",
    "# === Split train/val ===\n",
    "silver_pids = list(pid2labelids_silver.keys())\n",
    "train_pids, val_pids = train_test_split(silver_pids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_labels = {pid: pid2labelids_silver[pid] for pid in train_pids}\n",
    "val_labels = {pid: pid2labelids_silver[pid] for pid in val_pids}\n",
    "\n",
    "print(f\"Train: {len(train_labels)} | Val: {len(val_labels)}\")\n",
    "\n",
    "# === Filter valid ===\n",
    "valid_pids = set(pid2idx.keys())\n",
    "train_labels = {pid: labels for pid, labels in train_labels.items() if pid in valid_pids}\n",
    "val_labels = {pid: labels for pid, labels in val_labels.items() if pid in valid_pids}\n",
    "\n",
    "print(f\"âœ“ Train valid: {len(train_labels)} | Val valid: {len(val_labels)}\")\n",
    "missing = [pid for pid in silver_pids if pid not in pid2idx]\n",
    "print(f\"âš ï¸ Missing {len(missing)} PIDs (ex: {missing[:5]})\")\n",
    "\n",
    "# === Dataset & Loaders ===\n",
    "train_dataset = ProductCategoryDataset(train_labels, pid2idx, X_train, num_classes=NUM_CLASSES)\n",
    "val_dataset = ProductCategoryDataset(val_labels, pid2idx, X_train, num_classes=NUM_CLASSES)\n",
    "\n",
    "test_dataset = ProductCategoryDataset(\n",
    "    None,\n",
    "    {pid: i for i, pid in enumerate(test_ids)},\n",
    "    X_test,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "508e5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_example_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Example-F1 (Samples F1-score)\n",
    "    \n",
    "    Formula: Example-F1 = (1/N) Ã— Î£ [2Ã—|Ctrue âˆ© Cpred| / (|Ctrue| + |Cpred|)]\n",
    "    \n",
    "    Args:\n",
    "        y_true: array (n_samples, n_classes) - labels vrais (0 ou 1)\n",
    "        y_pred: array (n_samples, n_classes) - labels prÃ©dits (0 ou 1)\n",
    "    \n",
    "    Returns:\n",
    "        float: Example-F1 score entre 0 et 1\n",
    "    \"\"\"\n",
    "    f1_per_sample = []\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        true_set = set(np.where(y_true[i] == 1)[0])\n",
    "        pred_set = set(np.where(y_pred[i] == 1)[0])\n",
    "        \n",
    "        # Cas spÃ©ciaux\n",
    "        if len(true_set) == 0 and len(pred_set) == 0:\n",
    "            f1_per_sample.append(1.0)\n",
    "        elif len(true_set) == 0 or len(pred_set) == 0:\n",
    "            f1_per_sample.append(0.0)\n",
    "        else:\n",
    "            intersection = len(true_set & pred_set)\n",
    "            f1 = 2 * intersection / (len(true_set) + len(pred_set))\n",
    "            f1_per_sample.append(f1)\n",
    "    \n",
    "    return np.mean(f1_per_sample)\n",
    "\n",
    "def evaluate_complete(model, dataloader, device=\"cpu\", threshold=0.5):\n",
    "    \"\"\"\n",
    "    Standard evaluation function for multi-label classification.\n",
    "\n",
    "    Computes:\n",
    "      - example_f1 : average F1 per sample\n",
    "      - f1_macro   : unweighted class average\n",
    "      - f1_micro   : global average across all decisions\n",
    "      - f1_weighted: sample-weighted (by class frequency)\n",
    "      - exact_match: proportion of samples with all labels correct\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[\"X\"].to(device)\n",
    "            y = batch[\"y\"].cpu().numpy()\n",
    "\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs > threshold).astype(int)\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y)\n",
    "\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    example_f1 = compute_example_f1(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    f1_samples = f1_score(all_labels, all_preds, average=\"samples\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"example_f1\": example_f1,\n",
    "        \"sample_f1\": f1_samples,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_micro\": f1_micro\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d80597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 2,178,048\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "model = GCNEnhancedClassifier(\n",
    "    input_dim=input_dim,\n",
    "    label_init_emb=label_emb,\n",
    "    A_hat=A_hat,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Teacher model (EMA)\n",
    "teacher = copy.deepcopy(model).to(device)\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Total params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4590114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:03<00:00, 116.39it/s, loss=0.0111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0688\n",
      "Val F1: 0.0000\n",
      "Sample F1: 0.0000\n",
      "New best model\n",
      "\n",
      "=== Epoch 2/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 131.93it/s, loss=0.0097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0107\n",
      "Val F1: 0.6222\n",
      "Sample F1: 0.6222\n",
      "New best model\n",
      "\n",
      "=== Epoch 3/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 135.63it/s, loss=0.0093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0099\n",
      "Val F1: 0.6222\n",
      "Sample F1: 0.6222\n",
      "\n",
      "=== Epoch 4/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:03<00:00, 121.61it/s, loss=0.0107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0096\n",
      "Val F1: 0.6222\n",
      "Sample F1: 0.6222\n",
      "\n",
      "=== Epoch 5/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:03<00:00, 110.46it/s, loss=0.0077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0093\n",
      "Val F1: 0.6219\n",
      "Sample F1: 0.6219\n",
      "\n",
      "=== Epoch 6/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:03<00:00, 112.81it/s, loss=0.0082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0089\n",
      "Val F1: 0.6286\n",
      "Sample F1: 0.6286\n",
      "New best model\n",
      "\n",
      "=== Epoch 7/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:04<00:00, 81.16it/s, loss=0.0091] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0087\n",
      "Val F1: 0.6320\n",
      "Sample F1: 0.6320\n",
      "New best model\n",
      "\n",
      "=== Epoch 8/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:04<00:00, 76.29it/s, loss=0.0083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0085\n",
      "Val F1: 0.6354\n",
      "Sample F1: 0.6354\n",
      "New best model\n",
      "\n",
      "=== Epoch 9/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:04<00:00, 74.55it/s, loss=0.0068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0084\n",
      "Val F1: 0.6380\n",
      "Sample F1: 0.6380\n",
      "New best model\n",
      "\n",
      "=== Epoch 10/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:04<00:00, 74.03it/s, loss=0.0085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0083\n",
      "Val F1: 0.6413\n",
      "Sample F1: 0.6413\n",
      "New best model\n",
      "\n",
      "=== Epoch 11/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:04<00:00, 74.40it/s, loss=0.0087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0082\n",
      "Val F1: 0.6436\n",
      "Sample F1: 0.6436\n",
      "New best model\n",
      "\n",
      "=== Epoch 12/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:03<00:00, 95.67it/s, loss=0.0070] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0081\n",
      "Val F1: 0.6496\n",
      "Sample F1: 0.6496\n",
      "New best model\n",
      "\n",
      "=== Epoch 13/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:08<00:00, 42.10it/s, loss=0.0087] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0080\n",
      "Val F1: 0.6544\n",
      "Sample F1: 0.6544\n",
      "New best model\n",
      "\n",
      "=== Epoch 14/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:07<00:00, 47.96it/s, loss=0.0082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0079\n",
      "Val F1: 0.6581\n",
      "Sample F1: 0.6581\n",
      "New best model\n",
      "\n",
      "=== Epoch 15/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:08<00:00, 44.42it/s, loss=0.0084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0079\n",
      "Val F1: 0.6610\n",
      "Sample F1: 0.6610\n",
      "New best model\n",
      "\n",
      "=== Epoch 16/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:03<00:00, 117.98it/s, loss=0.0081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6636\n",
      "Sample F1: 0.6636\n",
      "New best model\n",
      "\n",
      "=== Epoch 17/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 131.24it/s, loss=0.0076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6645\n",
      "Sample F1: 0.6645\n",
      "New best model\n",
      "\n",
      "=== Epoch 18/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 132.47it/s, loss=0.0070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6664\n",
      "Sample F1: 0.6664\n",
      "New best model\n",
      "\n",
      "=== Epoch 19/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 132.40it/s, loss=0.0076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6654\n",
      "Sample F1: 0.6654\n",
      "\n",
      "=== Epoch 20/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 138.59it/s, loss=0.0074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6664\n",
      "Sample F1: 0.6664\n",
      "New best model\n",
      "\n",
      "=== Epoch 21/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 131.87it/s, loss=0.0087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6643\n",
      "Sample F1: 0.6643\n",
      "\n",
      "=== Epoch 22/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 136.20it/s, loss=0.0066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6670\n",
      "Sample F1: 0.6670\n",
      "New best model\n",
      "\n",
      "=== Epoch 23/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 135.01it/s, loss=0.0077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6668\n",
      "Sample F1: 0.6668\n",
      "\n",
      "=== Epoch 24/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 132.60it/s, loss=0.0077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0077\n",
      "Val F1: 0.6679\n",
      "Sample F1: 0.6679\n",
      "New best model\n",
      "\n",
      "=== Epoch 25/25 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 369/369 [00:02<00:00, 136.09it/s, loss=0.0075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078\n",
      "Val F1: 0.6685\n",
      "Sample F1: 0.6685\n",
      "New best model\n",
      "\n",
      "==================================================\n",
      "Best Val F1: 0.6685\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "alpha_ema = 0.995\n",
    "lambda_cons = 0.5\n",
    "best_val_f1 = -1\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "EPOCHS = 25\n",
    "\n",
    "# Tracking\n",
    "results_dict = {'valid': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n",
    "    \n",
    "    model.train()\n",
    "    teacher.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        X = batch[\"X\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        \n",
    "        # Student predictions\n",
    "        logits_student = model(X)\n",
    "        loss_sup = criterion(logits_student, y)\n",
    "        \n",
    "        # Teacher predictions\n",
    "        with torch.no_grad():\n",
    "            logits_teacher = teacher(X)\n",
    "        \n",
    "        # Consistency loss\n",
    "        loss_cons = F.mse_loss(torch.sigmoid(logits_student), torch.sigmoid(logits_teacher))\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_sup + lambda_cons * loss_cons\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update teacher (EMA)\n",
    "        with torch.no_grad():\n",
    "            for param_teacher, param_student in zip(teacher.parameters(), model.parameters()):\n",
    "                param_teacher.data = alpha_ema * param_teacher.data + (1 - alpha_ema) * param_student.data\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    print(f\"Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = evaluate_complete(teacher, val_loader, device=device)\n",
    "    val_f1 = val_metrics['example_f1']\n",
    "    f1_test = val_metrics['sample_f1']\n",
    "    results_dict['valid'].append(val_f1)\n",
    "    print(f\"Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Sample F1: {f1_test:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state = copy.deepcopy(teacher.state_dict())\n",
    "        patience_counter = 0\n",
    "        print(f\"New best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best Val F1: {best_val_f1:.4f}\")\n",
    "print(f\"{'='*50}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df5d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308/308 [00:00<00:00, 522.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Submission saved to Submission/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Charger le meilleur Ã©tat du modÃ¨le enseignant\n",
    "teacher.load_state_dict(best_model_state)\n",
    "teacher.eval()\n",
    "\n",
    "all_preds = []\n",
    "teacher.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting on test\"):\n",
    "        X = batch[\"X\"].to(device)\n",
    "        logits = teacher(X)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_preds.append(probs)\n",
    "\n",
    "# Fusionner les prÃ©dictions\n",
    "all_preds = np.vstack(all_preds)\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_labels = [np.where(row > threshold)[0].tolist() for row in all_preds]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "submission_rows = []\n",
    "for pid, labels in zip(test_ids, predicted_labels):\n",
    "    labels_str = \" \".join(map(str, labels))\n",
    "    submission_rows.append({\"id\": pid, \"label\": labels_str})\n",
    "\n",
    "df_submission = pd.DataFrame(submission_rows)\n",
    "df_submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"âœ… Submission saved to {SUBMISSION_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308/308 [00:01<00:00, 181.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19658 samples generated.\n",
      "âœ… Submission file saved: Submission/GNN.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv, os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X_test = torch.load(\"Embeddings/X_test.pt\").to(device)\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "MIN_LABELS = 2      # ðŸ‘ˆ au moins 2 labels\n",
    "MAX_LABELS = 3      # ðŸ‘ˆ au maximum 3 labels\n",
    "OUTPUT_PATH = \"Submission/GNN.csv\"\n",
    "os.makedirs(\"Submission\", exist_ok=True)\n",
    "\n",
    "all_pids, all_pred_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(X_test), 64), desc=\"Generating predictions\"):\n",
    "        end = start + 64\n",
    "        batch = X_test[start:end]\n",
    "        batch_pids = test_ids[start:end]\n",
    "\n",
    "        logits = teacher(batch)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        for pid, prob in zip(batch_pids, probs):\n",
    "            # prÃ©diction binaire selon le seuil\n",
    "            pred_row = (prob > THRESHOLD).astype(int)\n",
    "\n",
    "            # --- âš™ï¸ Aucune prÃ©diction : on prend le top-2 (minimum garanti)\n",
    "            if pred_row.sum() == 0:\n",
    "                topk_idx = np.argsort(prob)[-MIN_LABELS:][::-1]\n",
    "                pred_row = np.zeros_like(pred_row)\n",
    "                pred_row[topk_idx] = 1\n",
    "\n",
    "            # --- âš™ï¸ Seulement 1 label : on ajoute le deuxiÃ¨me plus probable\n",
    "            elif pred_row.sum() == 1:\n",
    "                top2_idx = np.argsort(prob)[-2:][::-1]\n",
    "                pred_row = np.zeros_like(pred_row)\n",
    "                pred_row[top2_idx] = 1\n",
    "\n",
    "            # --- âš™ï¸ Trop de labels : on garde seulement les top-3\n",
    "            elif pred_row.sum() > MAX_LABELS:\n",
    "                topk_idx = np.argsort(prob)[-MAX_LABELS:][::-1]\n",
    "                pred_row = np.zeros_like(pred_row)\n",
    "                pred_row[topk_idx] = 1\n",
    "\n",
    "            # --- Labels finaux\n",
    "            labels = [str(j) for j, v in enumerate(pred_row) if v == 1]\n",
    "\n",
    "            all_pids.append(pid)\n",
    "            all_pred_labels.append(labels)\n",
    "\n",
    "print(f\"{len(all_pids)} samples generated.\")\n",
    "\n",
    "# --- Sauvegarde CSV\n",
    "with open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"label\"])\n",
    "    for pid, labels in zip(all_pids, all_pred_labels):\n",
    "        writer.writerow([pid, \",\".join(labels)])\n",
    "\n",
    "print(f\"âœ… Submission file saved: {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
