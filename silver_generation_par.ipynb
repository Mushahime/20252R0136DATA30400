{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from utils import * \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Default paths\n",
    "ROOT = Path(\"Amazon_products\") # Root Amazon_products directory\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "TRAIN_CORPUS_PATH = os.path.join(TRAIN_DIR, \"train_corpus.txt\")\n",
    "\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\" \n",
    "CLASS_RELATED_PATH = ROOT / \"class_related_keywords.txt\" \n",
    "CLASS_PATH = ROOT / \"classes.txt\" \n",
    "\n",
    "SUBMISSION_PATH = \"Submission/submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0â€“530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# --- Load ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {id: text} dictionary.\"\"\"\n",
    "    id2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                id, text = parts\n",
    "                id2text[id] = text\n",
    "    return id2text\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"Load multi-label data into {id: [labels]} dictionary.\"\"\"\n",
    "    id2labels = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, label = parts\n",
    "                pid = int(pid)\n",
    "                label = int(label)\n",
    "\n",
    "                if pid not in id2labels:\n",
    "                    id2labels[pid] = []\n",
    "\n",
    "                id2labels[pid].append(label)\n",
    "    return id2labels\n",
    "\n",
    "def load_class_keywords(path):\n",
    "    \"\"\"Load class keywords into {class_name: [keywords]} dictionary.\"\"\"\n",
    "    class2keywords = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            classname, keywords = line.strip().split(\":\", 1)\n",
    "            keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
    "            class2keywords[classname] = keyword_list\n",
    "    return class2keywords\n",
    "\n",
    "id2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "id_list_test = list(id2text_test.keys())\n",
    "\n",
    "id2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "id_list_train = list(id2text_train.keys())\n",
    "\n",
    "id2class = load_corpus(CLASS_PATH)\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "class2related = load_class_keywords(CLASS_RELATED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c95411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_stats(name, silver):\n",
    "    counts = [len(v) for v in silver.values()]\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Documents: {len(counts)}\")\n",
    "    print(f\"  Avg labels/doc: {np.mean(counts):.2f}\")\n",
    "    print(f\"  Min labels: {np.min(counts)}\")\n",
    "    print(f\"  Max labels: {np.max(counts)}\")\n",
    "\n",
    "def hierarchy_consistency(silver, hierarchy):\n",
    "    ok = total = 0\n",
    "    for labels in silver.values():\n",
    "        L = set(labels)\n",
    "        for parent, children in hierarchy.items():\n",
    "            for child in children:\n",
    "                if child in L:\n",
    "                    total += 1\n",
    "                    if parent in L:\n",
    "                        ok += 1\n",
    "    return ok / total if total > 0 else 0\n",
    "\n",
    "def count_present_classes(silver, total_classes=531):\n",
    "    # Collect all unique labels appearing in the dataset\n",
    "    all_labels = set(label for labels in silver.values() for label in labels)\n",
    "    \n",
    "    # Count how many distinct classes are present\n",
    "    n_present = len(all_labels)\n",
    "    \n",
    "    print(f\"Present classes: {n_present}/{total_classes} ({n_present/total_classes*100:.2f}%)\")\n",
    "    return n_present\n",
    "\n",
    "from collections import Counter\n",
    "def analyze_coverage(silver, name):\n",
    "    all_labels = []\n",
    "    for info in silver.values():\n",
    "        all_labels.extend(info)\n",
    "    \n",
    "    unique = len(set(all_labels))\n",
    "    counter = Counter(all_labels)\n",
    "    top5 = counter.most_common(5)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Coverage: {unique}/531 ({unique/531*100:.1f}%)\")\n",
    "    print(f\"  Top-5 most frequent:\")\n",
    "    for cls, count in top5:\n",
    "        print(f\"    Class {cls}: {count} times ({count/len(silver)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ab0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_with_hierarchy(labels, hierarchy):\n",
    "    \"\"\"\n",
    "    Expand a list of core labels by adding ALL their ancestors\n",
    "    (parents, parents of parents, etc.), recursively.\n",
    "    This guarantees 100% hierarchy consistency.\n",
    "    \"\"\"\n",
    "    expanded = set(labels)\n",
    "    stack = list(labels)\n",
    "\n",
    "    # Build reverse parent â†’ children mapping\n",
    "    # hierarchy = { parent: [children] }\n",
    "    # We need the reverse: child â†’ parents\n",
    "    child2parents = {}\n",
    "    for parent, children in hierarchy.items():\n",
    "        for child in children:\n",
    "            child2parents.setdefault(child, []).append(parent)\n",
    "\n",
    "    # DFS / BFS upward through ancestors\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "\n",
    "        if node not in child2parents:\n",
    "            continue\n",
    "\n",
    "        for parent in child2parents[node]:\n",
    "            if parent not in expanded:\n",
    "                expanded.add(parent)\n",
    "                stack.append(parent)   # continue climbing up\n",
    "\n",
    "    return sorted(expanded)[-3:]\n",
    "\n",
    "\n",
    "def propagate_hierarchy_simple(\n",
    "    label_embeddings,\n",
    "    class_hierarchy,\n",
    "    alpha=0.7,\n",
    "    include_children=False,\n",
    "    normalize=True\n",
    "):\n",
    "    device = label_embeddings.device\n",
    "    num_classes = label_embeddings.shape[0]\n",
    "    updated = label_embeddings.clone()\n",
    "    \n",
    "    # Pass 1: Parents â†’ Children\n",
    "    for class_id in range(num_classes):\n",
    "        class_id_str = str(class_id)\n",
    "        \n",
    "        if class_id_str not in class_hierarchy:\n",
    "            continue\n",
    "        \n",
    "        parents = class_hierarchy[class_id_str].get(\"parents\", [])\n",
    "        valid_parents = [p for p in parents if 0 <= p < num_classes]\n",
    "        \n",
    "        if valid_parents:\n",
    "            parent_vec = label_embeddings[valid_parents].mean(dim=0)\n",
    "            updated[class_id] = (1 - alpha) * label_embeddings[class_id] + alpha * parent_vec\n",
    "    \n",
    "    # Pass 2: Children â†’ Parents\n",
    "    if include_children:\n",
    "        temp = updated.clone()\n",
    "        for class_id in range(num_classes):\n",
    "            class_id_str = str(class_id)\n",
    "            \n",
    "            if class_id_str not in class_hierarchy:\n",
    "                continue\n",
    "            \n",
    "            children = class_hierarchy[class_id_str].get(\"children\", [])\n",
    "            valid_children = [c for c in children if 0 <= c < num_classes]\n",
    "            \n",
    "            if valid_children:\n",
    "                children_vec = updated[valid_children].mean(dim=0)\n",
    "                temp[class_id] = (1 - alpha) * updated[class_id] + alpha * children_vec\n",
    "        \n",
    "        updated = temp\n",
    "    \n",
    "    # Normalize\n",
    "    if normalize:\n",
    "        norms = torch.norm(updated, dim=1, keepdim=True)\n",
    "        updated = updated / (norms + 1e-8)\n",
    "    \n",
    "    return updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51943b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model, batch_size=64, save_path=None, force_recompute=False):\n",
    "\n",
    "    # Load cache\n",
    "    if save_path and os.path.exists(save_path) and not force_recompute:\n",
    "        print(f\"ðŸ“¦ Loading from {save_path}\")\n",
    "        emb = torch.load(save_path, map_location=\"cpu\")\n",
    "        if isinstance(emb, np.ndarray):\n",
    "            emb = torch.from_numpy(emb)\n",
    "        return emb\n",
    "\n",
    "    print(f\"âš™ï¸ Encoding {len(texts)} texts on {model.device}...\")\n",
    "\n",
    "    # ---- ENCODE ----\n",
    "    emb = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,  \n",
    "        device=model.device\n",
    "    )\n",
    "\n",
    "    # ---- SAFETY CHECK ----\n",
    "    if isinstance(emb, list):\n",
    "        emb = torch.stack([e for e in emb])  \n",
    "\n",
    "    elif isinstance(emb, np.ndarray):\n",
    "        emb = torch.from_numpy(emb)\n",
    "\n",
    "    # ---- CPU for saving ----\n",
    "    emb = emb.cpu()\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        torch.save(emb, save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    return emb\n",
    "\n",
    "def get_enriched_category_with_hierarchy(class_id, id2class, class2related, class_hierarchy, max_keywords=10):\n",
    "    class_name = id2class[str(class_id)]\n",
    "    clean_name = class_name.replace('_', ' ')\n",
    "    \n",
    "    # Parents\n",
    "    parents = class_hierarchy.get(str(class_id), {}).get(\"parents\", [])\n",
    "    parent_names = []\n",
    "    for p in parents:\n",
    "        if 0 <= p < 531:\n",
    "            parent_name = id2class[str(p)].replace('_', ' ')\n",
    "            if parent_name.lower() != \"root\":\n",
    "                parent_names.append(parent_name)\n",
    "    \n",
    "    # Keywords\n",
    "    keywords = class2related.get(class_name, [])[:max_keywords]\n",
    "    \n",
    "    # Combine\n",
    "    parts = [clean_name]\n",
    "    if parent_names:\n",
    "        parts.extend(parent_names)\n",
    "    if keywords:\n",
    "        parts.extend(keywords)\n",
    "    \n",
    "    return \" \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536aff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2896d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_silver_labels_FAST(\n",
    "    train_texts,\n",
    "    train_ids,\n",
    "    test_texts,\n",
    "    test_ids,\n",
    "    id2class,\n",
    "    class2related,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    class_hierarchy,\n",
    "    output_path_train=\"Silver/silver_train_new_para.json\",\n",
    "    output_path_test=\"Silver/silver_test_new_para.json\"\n",
    "):\n",
    "    \n",
    "    all_texts = train_texts + test_texts\n",
    "    all_ids = train_ids + test_ids\n",
    "\n",
    "    enriched_categories = [\n",
    "        get_enriched_category_with_hierarchy(i, id2class, class2related, class_hierarchy)\n",
    "        for i in tqdm(range(531), desc=\"Enriching\")\n",
    "    ]\n",
    "\n",
    "    base_category_embeddings = get_embeddings(\n",
    "        enriched_categories,\n",
    "        model=model,\n",
    "        batch_size=64,\n",
    "        save_path=\"Embeddings/labels_base_new_para.pt\",\n",
    "        force_recompute=True\n",
    "    )\n",
    "\n",
    "    hierarchy_int = {}\n",
    "    for cid, rel in class_hierarchy.items():\n",
    "        parents = rel.get(\"parents\", []) if isinstance(rel, dict) else []\n",
    "        children = rel.get(\"children\", []) if isinstance(rel, dict) else rel if isinstance(rel, list) else []\n",
    "        hierarchy_int[cid] = {\"parents\": parents, \"children\": children}\n",
    "\n",
    "    # Ensure tensor format\n",
    "    if isinstance(base_category_embeddings, list):\n",
    "        print(\"base_category_embeddings is list â†’ stacking\")\n",
    "        base_category_embeddings = torch.stack(base_category_embeddings)\n",
    "    elif isinstance(base_category_embeddings, np.ndarray):\n",
    "        print(\"base_category_embeddings is numpy â†’ converting\")\n",
    "        base_category_embeddings = torch.from_numpy(base_category_embeddings)\n",
    "    elif isinstance(base_category_embeddings, torch.Tensor):\n",
    "        print(\"base_category_embeddings is already torch\")\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected type: {type(base_category_embeddings)}\")\n",
    "    \n",
    "    hierarchical_embeddings = propagate_hierarchy_simple(\n",
    "        label_embeddings=base_category_embeddings,\n",
    "        class_hierarchy=hierarchy_int,\n",
    "        alpha=0.7,\n",
    "        include_children=False,\n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "    torch.save(hierarchical_embeddings, \"Embeddings/labels_hierarchical_new_para.pt\")\n",
    "\n",
    "    review_embeddings = get_embeddings(\n",
    "        all_texts,\n",
    "        model=model,\n",
    "        batch_size=64,\n",
    "        save_path=\"Embeddings/X_train_test_para.pt\",\n",
    "        force_recompute=True\n",
    "    )\n",
    "\n",
    "    # Ensure both are tensors\n",
    "    if isinstance(review_embeddings, np.ndarray):\n",
    "        review_embeddings = torch.from_numpy(review_embeddings)\n",
    "    if isinstance(hierarchical_embeddings, np.ndarray):\n",
    "        hierarchical_embeddings = torch.from_numpy(hierarchical_embeddings)\n",
    "\n",
    "    # Move to device for computation\n",
    "    review_embeddings = review_embeddings.to(device)\n",
    "    hierarchical_embeddings = hierarchical_embeddings.to(device)\n",
    "\n",
    "    # Compute similarity on device\n",
    "    all_similarities = torch.matmul(\n",
    "        review_embeddings,\n",
    "        hierarchical_embeddings.T\n",
    "    )\n",
    "\n",
    "    # Move back to CPU for numpy operations\n",
    "    all_similarities = all_similarities.cpu()\n",
    "\n",
    "    all_similarities2 = torch.matmul(\n",
    "    review_embeddings,\n",
    "    base_category_embeddings.to(device).T  # raw original embeddings\n",
    "    )\n",
    "    all_similarities2 = all_similarities2.cpu()\n",
    "\n",
    "    silver_train, silver_test = {}, {}\n",
    "    silver_train_nohier, silver_test_nohier = {}, {}\n",
    "\n",
    "    n_train = len(train_ids)\n",
    "\n",
    "    for idx, rid in enumerate(tqdm(all_ids, desc=\"Assigning\")):\n",
    "\n",
    "        sims = all_similarities[idx]\n",
    "        topk_scores, topk_idx = torch.topk(sims, k=1)\n",
    "        \n",
    "        topk_idx = topk_idx.tolist()\n",
    "        topk_scores = topk_scores.tolist()\n",
    "\n",
    "        expanded = expand_with_hierarchy(topk_idx, class_hierarchy)\n",
    "\n",
    "        expanded_scores = [float(sims[l]) for l in expanded]\n",
    "\n",
    "        sorted_labels = [\n",
    "            x for x, _ in sorted(\n",
    "                zip(expanded, expanded_scores),\n",
    "                key=lambda t: t[1],\n",
    "                reverse=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        sorted_scores = [\n",
    "            x for _, x in sorted(\n",
    "                zip(expanded, expanded_scores),\n",
    "                key=lambda t: t[1],\n",
    "                reverse=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        final_labels = sorted_labels\n",
    "        final_scores = sorted_scores\n",
    "        final_probs = torch.sigmoid(torch.tensor(final_scores)).tolist()\n",
    "\n",
    "        record = {\n",
    "            \"labels\": final_labels,\n",
    "            \"scores\": final_scores,\n",
    "            \"probs\": final_probs\n",
    "        }\n",
    "\n",
    "        if idx < n_train:\n",
    "            silver_train[rid] = record\n",
    "        else:\n",
    "            silver_test[rid] = record\n",
    "\n",
    "\n",
    "        sims2 = all_similarities2[idx]\n",
    "        topk_scores2, topk_idx2 = torch.topk(sims2, k=1)\n",
    "        topk_idx2 = topk_idx2.tolist()\n",
    "\n",
    "        expanded2 = expand_with_hierarchy(topk_idx2, class_hierarchy)\n",
    "        expanded_scores2 = [float(sims2[l]) for l in expanded2]\n",
    "\n",
    "        sorted_labels2 = [\n",
    "            x for x, _ in sorted(\n",
    "                zip(expanded2, expanded_scores2),\n",
    "                key=lambda t: t[1],\n",
    "                reverse=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        sorted_scores2 = [\n",
    "            x for _, x in sorted(\n",
    "                zip(expanded2, expanded_scores2),\n",
    "                key=lambda t: t[1],\n",
    "                reverse=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        record_nohier = {\n",
    "            \"labels\": sorted_labels2,\n",
    "            \"scores\": sorted_scores2,\n",
    "            \"probs\": torch.sigmoid(torch.tensor(sorted_scores2)).tolist()\n",
    "        }\n",
    "\n",
    "        if idx < n_train:\n",
    "            silver_train_nohier[rid] = record_nohier\n",
    "        else:\n",
    "            silver_test_nohier[rid] = record_nohier\n",
    "\n",
    "    os.makedirs(\"Silver\", exist_ok=True)\n",
    "\n",
    "    json.dump(silver_train, open(output_path_train, \"w\", encoding=\"utf-8\"), indent=2, ensure_ascii=False)\n",
    "    json.dump(silver_test, open(output_path_test, \"w\", encoding=\"utf-8\"), indent=2, ensure_ascii=False)\n",
    "\n",
    "    json.dump(silver_train_nohier,\n",
    "          open(\"Silver/silver_train_new_para_nohier.json\", \"w\", encoding=\"utf-8\"),\n",
    "          indent=2, ensure_ascii=False)\n",
    "\n",
    "    json.dump(silver_test_nohier,\n",
    "            open(\"Silver/silver_test_new_para_nohier.json\", \"w\", encoding=\"utf-8\"),\n",
    "            indent=2, ensure_ascii=False)\n",
    "\n",
    "    return silver_train, silver_test, silver_train_nohier, silver_test_nohier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016e589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATING TRAIN SILVER LABELS (FAST)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Encoding 531 texts on cuda:0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to Embeddings/labels_base_new_para.pt\n",
      "base_category_embeddings is already torch\n",
      "âš™ï¸ Encoding 49145 texts on cuda:0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [02:02<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to Embeddings/X_train_test_para.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49145/49145 [00:09<00:00, 5133.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Safe Train\n",
      "  Documents: 29487\n",
      "  Avg labels/doc: 3.00\n",
      "  Min labels: 3\n",
      "  Max labels: 3\n",
      "\n",
      "Safe Train\n",
      "  Documents: 29487\n",
      "  Avg labels/doc: 3.00\n",
      "  Min labels: 3\n",
      "  Max labels: 3\n",
      "\n",
      "Hierarchy Consistency: 94.16%\n",
      "\n",
      "Hierarchy Consistency: 95.44%\n",
      "Coverage: 94.73%\n",
      "Covered classes: 503/531\n",
      "Coverage: 89.27%\n",
      "Covered classes: 474/531\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Exec\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING TRAIN SILVER LABELS (FAST)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "silver_train_safe, silver_test_safe, silver_train_safe_nohier, silver_test_safe_nohier = generate_silver_labels_FAST(\n",
    "    list(id2text_train.values()),\n",
    "    list(id2text_train.keys()),\n",
    "    list(id2text_test.values()),\n",
    "    list(id2text_test.keys()),\n",
    "    id2class,\n",
    "    class2related,\n",
    "    None,\n",
    "    model,\n",
    "    class2hierarchy,\n",
    "    output_path_train=\"Silver/silver_train_new_para.json\",\n",
    "    output_path_test=\"Silver/silver_test_new_para.json\"\n",
    ")\n",
    "\n",
    "# Stats\n",
    "print()\n",
    "label_stats(\"Safe Train\", silver_train_safe)\n",
    "\n",
    "silver_train_labels_only = {\n",
    "    pid: info[\"labels\"]\n",
    "    for pid, info in silver_train_safe.items()\n",
    "}\n",
    "\n",
    "label_stats(\"Safe Train\", silver_train_safe_nohier)\n",
    "\n",
    "silver_train_labels_only_nohier = {\n",
    "    pid: info[\"labels\"]\n",
    "    for pid, info in silver_train_safe_nohier.items()\n",
    "}\n",
    "\n",
    "\n",
    "consistency = hierarchy_consistency(silver_train_labels_only, class2hierarchy)\n",
    "print(f\"\\nHierarchy Consistency: {consistency:.2%}\")\n",
    "\n",
    "consistency = hierarchy_consistency(silver_train_labels_only_nohier, class2hierarchy)\n",
    "print(f\"\\nHierarchy Consistency: {consistency:.2%}\")\n",
    "\n",
    "\n",
    "def label_coverage(silver_labels, num_classes=531):\n",
    "    \"\"\"\n",
    "    silver_labels : { review_id: [label1, label2, ...] }\n",
    "    returns coverage_ratio, covered_classes\n",
    "    \"\"\"\n",
    "    covered = set()\n",
    "\n",
    "    for _, labels in silver_labels.items():\n",
    "        for lbl in labels:\n",
    "            if 0 <= lbl < num_classes:\n",
    "                covered.add(lbl)\n",
    "\n",
    "    coverage_ratio = len(covered) / num_classes\n",
    "    return coverage_ratio, sorted(list(covered))\n",
    "\n",
    "coverage, classes = label_coverage(silver_train_labels_only)\n",
    "print(f\"Coverage: {coverage:.2%}\")\n",
    "print(f\"Covered classes: {len(classes)}/{531}\")\n",
    "\n",
    "coverage, classes = label_coverage(silver_train_labels_only_nohier)\n",
    "print(f\"Coverage: {coverage:.2%}\")\n",
    "print(f\"Covered classes: {len(classes)}/{531}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
