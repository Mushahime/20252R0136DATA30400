{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from utils import * \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33d512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths\n",
    "ROOT = Path(\"Amazon_products\") # Root Amazon_products directory\n",
    "TRAIN_DIR = ROOT / \"train\"\n",
    "TEST_DIR = ROOT / \"test\"\n",
    "\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "TRAIN_CORPUS_PATH = os.path.join(TRAIN_DIR, \"train_corpus.txt\")\n",
    "\n",
    "CLASS_HIERARCHY_PATH = ROOT / \"class_hierarchy.txt\" \n",
    "CLASS_RELATED_PATH = ROOT / \"class_related_keywords.txt\" \n",
    "CLASS_PATH = ROOT / \"classes.txt\" \n",
    "\n",
    "SUBMISSION_PATH = \"Submission/submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0‚Äì530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257bd473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {id: text} dictionary.\"\"\"\n",
    "    id2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                id, text = parts\n",
    "                id2text[id] = text\n",
    "    return id2text\n",
    "\n",
    "def load_multilabel(path):\n",
    "    \"\"\"Load multi-label data into {id: [labels]} dictionary.\"\"\"\n",
    "    id2labels = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, label = parts\n",
    "                pid = int(pid)\n",
    "                label = int(label)\n",
    "\n",
    "                if pid not in id2labels:\n",
    "                    id2labels[pid] = []\n",
    "\n",
    "                id2labels[pid].append(label)\n",
    "    return id2labels\n",
    "\n",
    "def load_class_keywords(path):\n",
    "    \"\"\"Load class keywords into {class_name: [keywords]} dictionary.\"\"\"\n",
    "    class2keywords = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            classname, keywords = line.strip().split(\":\", 1)\n",
    "            keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
    "            class2keywords[classname] = keyword_list\n",
    "    return class2keywords\n",
    "\n",
    "id2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "id_list_test = list(id2text_test.keys())\n",
    "\n",
    "id2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "id_list_train = list(id2text_train.keys())\n",
    "\n",
    "id2class = load_corpus(CLASS_PATH)\n",
    "class2hierarchy = load_multilabel(CLASS_HIERARCHY_PATH)\n",
    "class2related = load_class_keywords(CLASS_RELATED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ec71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_stats(name, silver):\n",
    "    counts = [len(v) for v in silver.values()]\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Documents: {len(counts)}\")\n",
    "    print(f\"  Avg labels/doc: {np.mean(counts):.2f}\")\n",
    "    print(f\"  Min labels: {np.min(counts)}\")\n",
    "    print(f\"  Max labels: {np.max(counts)}\")\n",
    "\n",
    "def hierarchy_consistency(silver, hierarchy):\n",
    "    ok = total = 0\n",
    "    for labels in silver.values():\n",
    "        L = set(labels)\n",
    "        for parent, children in hierarchy.items():\n",
    "            for child in children:\n",
    "                if child in L:\n",
    "                    total += 1\n",
    "                    if parent in L:\n",
    "                        ok += 1\n",
    "    return ok / total if total > 0 else 0\n",
    "\n",
    "def count_present_classes(silver, total_classes=531):\n",
    "    # Collect all unique labels appearing in the dataset\n",
    "    all_labels = set(label for labels in silver.values() for label in labels)\n",
    "    \n",
    "    # Count how many distinct classes are present\n",
    "    n_present = len(all_labels)\n",
    "    \n",
    "    print(f\"Present classes: {n_present}/{total_classes} ({n_present/total_classes*100:.2f}%)\")\n",
    "    return n_present\n",
    "\n",
    "def analyze_coverage(silver, name):\n",
    "    all_labels = []\n",
    "    for info in silver.values():\n",
    "        all_labels.extend(info)\n",
    "    \n",
    "    unique = len(set(all_labels))\n",
    "    counter = Counter(all_labels)\n",
    "    top5 = counter.most_common(5)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Coverage: {unique}/531 ({unique/531*100:.1f}%)\")\n",
    "    print(f\"  Top-5 most frequent:\")\n",
    "    for cls, count in top5:\n",
    "        print(f\"    Class {cls}: {count} times ({count/len(silver)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c29d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_with_hierarchy(core_labels, hierarchy):\n",
    "    \"\"\"Add parents in the hierarchy\"\"\"\n",
    "    expanded = set(core_labels)\n",
    "    \n",
    "    for label in core_labels:\n",
    "        for parent, children in hierarchy.items():\n",
    "            if label in children:\n",
    "                expanded.add(parent)\n",
    "                expanded.update(expand_with_hierarchy([parent], hierarchy))\n",
    "    \n",
    "    # Keep 3 more specific\n",
    "    return sorted(list(expanded))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76444fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading model: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "print(f\"üß† Loading model: {model_name}\")\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "def get_embeddings(texts, model, batch_size=64, save_path=None, force_recompute=False):\n",
    "    \n",
    "    if save_path and os.path.exists(save_path) and not force_recompute:\n",
    "        print(f\"üì¶ Loading precomputed embeddings from {save_path}\")\n",
    "        return torch.load(save_path, map_location=\"cpu\")\n",
    "\n",
    "    print(f\"‚öôÔ∏è Encoding {len(texts)} texts...\")\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        torch.save(embeddings, save_path)\n",
    "        print(f\"üíæ Saved embeddings to {save_path}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_enriched_category_text(class_id, id2class, class2related, max_keywords=10):\n",
    "    class_name = id2class[str(class_id)]\n",
    "    \n",
    "    # Replace underscore with space for better understanding\n",
    "    clean_name = class_name.replace('_', ' ')\n",
    "    \n",
    "    # Add keywords if available\n",
    "    if class_name in class2related:\n",
    "        keywords = class2related[class_name][:max_keywords]\n",
    "        keywords_str = \" \".join(keywords)\n",
    "        enriched = f\"{clean_name} {keywords_str}\"\n",
    "    else:\n",
    "        enriched = clean_name\n",
    "    \n",
    "    return enriched\n",
    "\n",
    "\n",
    "def generate_silver_labels_FAST(\n",
    "    review_texts,\n",
    "    review_ids,\n",
    "    id2class,\n",
    "    class2related,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    class_hierarchy,\n",
    "    output_path=\"Silver/silver_train_true.json\"\n",
    "):\n",
    "    \n",
    "    # Create enriched category descriptions\n",
    "    enriched_categories = [\n",
    "        get_enriched_category_text(i, id2class, class2related)\n",
    "        for i in tqdm(range(531), desc=\"Enriching\")\n",
    "    ]\n",
    "\n",
    "    category_embeddings = get_embeddings(\n",
    "        enriched_categories,\n",
    "        model = model,\n",
    "        batch_size=64,\n",
    "        save_path=\"Embeddings/labels_true.pt\",\n",
    "        force_recompute=True\n",
    "    )\n",
    "\n",
    "    review_embeddings = get_embeddings(\n",
    "        review_texts,\n",
    "        model = model,\n",
    "        batch_size=64,\n",
    "        save_path=\"Embeddings/X_train_true.pt\",\n",
    "        force_recompute=True\n",
    "    )\n",
    "\n",
    "    def to_numpy(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.detach().cpu().numpy()\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            return x\n",
    "        elif isinstance(x, list):\n",
    "            return np.array(x)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported type for embeddings: {type(x)}\")\n",
    "\n",
    "    category_embeddings_np = to_numpy(category_embeddings)\n",
    "    review_embeddings_np = to_numpy(review_embeddings)\n",
    "\n",
    "    all_similarities = torch.matmul(\n",
    "        torch.tensor(review_embeddings_np),\n",
    "        torch.tensor(category_embeddings_np).T\n",
    "    ).cpu().numpy()\n",
    "\n",
    "    # Assign labels\n",
    "    silver_labels = {}\n",
    "    \n",
    "    for idx, review_id in enumerate(tqdm(review_ids, desc=\"Labels\")):\n",
    "        similarities = all_similarities[idx]\n",
    "        \n",
    "        # Top-3 classes et scores\n",
    "        topk_idx = np.argsort(similarities)[-3:][::-1]\n",
    "        topk_scores = similarities[topk_idx]\n",
    "        \n",
    "        # Expansion with hierarchy\n",
    "        expanded = expand_with_hierarchy(topk_idx.tolist(), class_hierarchy)\n",
    "        \n",
    "        # Normalized pseudo-probabilities (0‚Äì1)\n",
    "        pseudo_probs = ((topk_scores + 1) / 2).tolist()\n",
    "        \n",
    "        # Save everything\n",
    "        silver_labels[review_id] = {\n",
    "            \"labels\": expanded[:3],\n",
    "            \"scores\": topk_scores[:3].tolist(),\n",
    "            \"probs\": pseudo_probs[:3]\n",
    "        }\n",
    "    \n",
    "    # Save as JSON\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(silver_labels, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Silver labels saved to: {output_path}\")\n",
    "    return silver_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94668757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATING TRAIN SILVER LABELS (FAST)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 531/531 [00:00<00:00, 1299402.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Encoding 531 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved embeddings to Embeddings/labels_true.pt\n",
      "‚öôÔ∏è Encoding 29487 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461/461 [01:15<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved embeddings to Embeddings/X_train_true.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29487/29487 [00:02<00:00, 13364.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver labels saved to: Silver/silver_train_true.json\n",
      "‚öôÔ∏è Encoding 19658 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 308/308 [00:55<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved embeddings to Embeddings/X_test_noam.pt\n",
      "\n",
      "\n",
      "Safe Train\n",
      "  Documents: 29487\n",
      "  Avg labels/doc: 3.00\n",
      "  Min labels: 3\n",
      "  Max labels: 3\n"
     ]
    }
   ],
   "source": [
    "# Exec\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING TRAIN SILVER LABELS (FAST)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "silver_train_safe = generate_silver_labels_FAST(\n",
    "    list(id2text_train.values()),\n",
    "    list(id2text_train.keys()),\n",
    "    id2class,\n",
    "    class2related,\n",
    "    None,\n",
    "    model,\n",
    "    class2hierarchy,\n",
    "    output_path = \"Silver/silver_train_true.json\"\n",
    ")\n",
    "\n",
    "get_embeddings(list(id2text_test.values()), model=model, batch_size=64, save_path=\"Embeddings/X_test_true.pt\", force_recompute=True)\n",
    "\n",
    "# Stats\n",
    "print()\n",
    "label_stats(\"Safe Train\", silver_train_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4252737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hierarchy Consistency: 98.96%\n"
     ]
    }
   ],
   "source": [
    "silver_train_labels_only = {\n",
    "    pid: info[\"labels\"]\n",
    "    for pid, info in silver_train_safe.items()\n",
    "}\n",
    "\n",
    "consistency = hierarchy_consistency(silver_train_labels_only, class2hierarchy)\n",
    "print(f\"\\nHierarchy Consistency: {consistency:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81411b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 50.47%\n",
      "Covered classes: 268/531\n"
     ]
    }
   ],
   "source": [
    "def label_coverage(silver_labels, num_classes=531):\n",
    "    \"\"\"\n",
    "    silver_labels : { review_id: [label1, label2, ...] }\n",
    "    returns coverage_ratio, covered_classes\n",
    "    \"\"\"\n",
    "    covered = set()\n",
    "\n",
    "    for _, labels in silver_labels.items():\n",
    "        for lbl in labels:\n",
    "            if 0 <= lbl < num_classes:\n",
    "                covered.add(lbl)\n",
    "\n",
    "    coverage_ratio = len(covered) / num_classes\n",
    "    return coverage_ratio, sorted(list(covered))\n",
    "\n",
    "coverage, classes = label_coverage(silver_train_labels_only)\n",
    "print(f\"Coverage: {coverage:.2%}\")\n",
    "print(f\"Covered classes: {len(classes)}/{531}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a90a22",
   "metadata": {},
   "source": [
    "# TODO faire test + train et labels embeddings avec hierarchy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
